Google Cloud Fundamentals: Core Infrastructure


Learning objectives
	1. Identify the purpose and value of Google Cloud products and services
	2. Define how infrastructure is organized and controlled in Google Cloud
	3. Explain how to create a basic infrastructure in google Cloud
	4. Select and use Google Cloud storage options
	5. Describe the purpose and value of Google Kubernetes Engine
	6. Identify the use cases for serverless Google Cloud services
	

Week 1 Course Introduction

Learning Objectives
	1. Welcome learners to the course.

Week 2 Introducing Google Cloud

Learning objectives
	1. Identify the benefits of Google Cloud.
	2. Define the components of the Google network infrastructure, including points of presence, data centers, regions, and zones.
	3. Identify the difference between infrastructure as a service (IaaS) and platform as a service (PaaS).

Cloud Computing Overview
	- way of using IT that has 5 equal important traits
		1. On demand and self service
		2. Access those resources from anywhere over the internet
		3. Provider allocates resources to users out of the pool
		4. Resources are elastic means flexible
		5. Pay what you use/reserver as they go

Why in demand nowadays?
	History
		First wave
			colocation
				- users rents physical space of the DC provider without investing own data center
		
		Second wave
			virtualized data center
				- uses virtual devices, match physical building blocks(CPUs, Disks, network) of hosted computing resources
				- enterprieses still maintain infra and user controleed/user-config env
				- not fast enough cuz not automated

		Third wave
			Container-based architecture
				- fully automated elastic(auto scale) means automated service and scalable data
				- auto provision and config infra used to run app

		Future wave
			Data Company
				- great software based on high quality data


IaaS and PaaS
	IaaS
	 	- cloud manage:
	 		raw compute/server/virtualization
	 		storage
	 		newtork
	 	- you:
	 		app
	 		data
	 		runtime
	 		middleware
	 		OS

	 	Sample
	 		Compute Enigine

	PaaS
		- cloud manage:
	 		everything
	 	- you:
	 		app
	 		data
	 	Sample
	 		App engine

	 Serverless/Function as a service
	 	AWS/GCP run containers for you based on CPU/RAM you need
	 	you don't manage/provision servers so you can concentrate on code
	 	lambda/cloud function
	 		even-drivent driven code
	 			pay as you go
	 	cloud run/ECS-Fargate

	 SaaS
	 	- cloud manage all
	 	app run in the cloud as a service
	 	Sample
	 		gmail
	 		docs
	 		drive

The Google Cloud network
	Region(40)/AWS Region(33)
		singapore

	Zone(121)/Availability-zone(105)
		asia-souteast1a-c

	Network Edge locations(187)/Edge Locations(400)

	Sample
		Multi region archi sample
		Cloud Spanbner/aurora

Environment Impact
	ISO 14001
	carbon neutral/carbon free

Security
	Hardware infra layer
		own design  chips/own network hardware design
		Secure boot stack
			ensures correct booting correct software stack
				crypto sign over BIOS
					bridge bet OS and hardware during startup process
					startup guide
				bootloader
					manage flow bet software and  hardware during starup process
					traffic cop
				kernel
					boss
					core component of OSmake sures sys resources, comm bet  hardware and software, and process runs smoothly
				OS of image

		Premises security

	Service deployment layer
		Encryption of inter-service communication
			provides crypto privacya nd remote procedure call(RPC) data on network

			RPC
				used when one comp talks to other comp over internet
				protocol use to enable program to exec code of another computer
				remote control

	User Identity layer
		User Identity
			google central identity service utilizes U2F

	Storage services layer
		Encyption at rest
			centrally managed keys
			SSD/hard drives
				support hardware encryption

	Internet Communication layer
		Google front end
			support perfect forward secrecy
				uses one time only session key
			support TLS uses public private key pair
			uses X.509 cert from CA

		DoS protection with cloud armor in Global external LB

	Operational security layer
		Intrusion detection
			ueses machine intel
			test penetration with red team exercises
		Reducing insider risk
			continuous limit employee access and montiors them
		Employee universal second factor(U2F) use
			U2F
				2 factor authentication like MFA
				prevents phishing attacks
		Software development practices
			practices
				central souce control
				two party cid review
				lib that prevent code class security bugs
				runs vulnerability rewards programs
					pay anyone who can discover nugs in the infra

Open source ecosystems
	can collaborate with many open sources like kb8, mongodb, tensorflow

	TensorFow
		open source library for ML

	Operations suits
		can monitor workloads across multiple cloud providers

Pricing and billing
	kb8s
		per seconnd billing

	Instance run more than 25% of a ,onth
		discount for every incremental min use fot that inca

		POnline pricing calculator

	Cloud Billing is AWS budget
		can create alert with budget limit met
		can be set with 50,90,100

	Cloud quotas
		 manage qoutas fir each cloud services
		 prevent sover consumption of resource

		Rate quotas
			project level

		Allocation qoutas
			project leve


Quiz
1. What is the primary benefit to a Google Cloud customer of using resources in several zones within a region?
	- For improved fault tolerance/availability

2.What type of cloud computing service lets you bind your application code to libraries that give access to the infrastructure your application needs?
	- PaaS(cloud run/build/app engine)

3.Why might a Google Cloud customer use resources in several regions around the world?
	- To bring their applications closer to users around the world, and for improved fault tolerance


Week 3 Resources and Access in the cloud

Learning Objectives
	1. Define the purpose of and use cases for IAM.
	2. List the methods of interaction with Google Cloud.
	3. Use Cloud Marketplace to interact with Google Cloud.
	4. Identify the purpose of projects on Google Cloud.

Google CLoud resource hierachy
	- directly relates to how policies are managed and applioed when you google cloud
	- policies are inherited downward
	- policies defined in
		org node
		folder
		project
Four level hierarchy
	1. resources
		VM in GCE
		buckets in GCS
		tables in bigquery

	2. Projects
		org in folders or subfolders
		policy here can enbale and use 
			services, 
			api, 
			billing, 
			add/remove collaborator
		can have diff owners and users
			billed and managed serpately

		attributes
			project id
				globally unique and can't be changed
				immutable
					used by google of exact project to work with
			proj name
				user created
				can be changed anythime
			proj number
				unique proj number 
				used by google to keep track resources

	3. Folders
		- allows can be used to group different department
		give team allow delgate admin rights for wokring independelty

	4. Org node that encompasses all three above

Cloud resource manager
	manage projets
	API that can gather list of all projects associateds with an an coount
	create new proj
	update exist project
	delete projects
	can recover deleted prokects
	- can be access through RPC and REST API

Project creator role
	conrol who can create projects means who can spend money

Workspace domain/org node
	projects will belong to this domain

Creating org node
	workspace domain
	cloud identity
	google identity
	acceess
	application
	endpoint mngmt platform

Identity and Access Management(IAM)
	power to restrict authorizations/access

	IAM policy
		set by admin
		a set of rules who can access resources
		
		who
			account
			group
			service account
			cloud identity domain
			- principal
				email add
		can do whaat
			role
				contains permissions
				
				Three kinds 
					1. basic
						affect all resources in the project

						proj viewer
							can accesss without edit
						prok owner
							can access and edit
							can manage broles and permission and set up billing
						proj editor
							can access and edit
						proj bill admin
							contorl billing of project and change resource in project
					2. predefined
						like instanceAdmin in GCE
					3. custom
						assign role that has specific permission
						can only be applied to project or org level
			why
				easier to mange

		principle
			zero trust security or least privilege permission


Service accounts
	-non human account withou human login that can access resources on behalf of syst/app

	Use case
		want only GCE VM to access cloud storage


Cloud Identity
	centrally managed users account like disabling them/removing from the group access to team's resources with google admin console

	- already abailbe to google workspace customer

	Active Director/LDAP(lightweight dir access protocol) sys
		a phone book that track who's who and their access bet computers and apps over a network


Interacting with Google Cloud
Four ways to access resources
	1. Google cloud console/web browser
	2. Cloud SDK and CLoud Shell
		Cloud SDK
			cli in local machine
			like gcloud and bq/bigquery
			- can manage google cloud resources
		Cloud Shell
			ClI in cloud console
			5 gb home dir
			debian VM
			cloud sdk gcloud pre installed
	3. APis
		allow code to be written to conrol services
		GOogle APis explorer
			shows what APis available in a resources
	4. Google Cloud app
		provides up to date bill info and alerts if budget is met
		can setup costom graphs of cpu, network/request per sercond

Coursera: Getting started with Google cloud platform and Qwiklabs
Lab
	Console and Cloud shell

Getting Started with Cloud marketplace
	sudo sh -c 'echo "<?php phpinfo(); ?>" > apache2/htdocs/phpinfo.php'

Quiz
1. Choose the correct completion: Services and APIs are enabled on a per-__________ basis.
- projects

2. Order these IAM role types from broadest to finest-grained.
- Basic roles, predefined roles, custom roles

3. Which of these values is globally unique, permanent, and unchangeable, but can be modified by the customer during creation?
- The project ID


Week 4 Virtual Machines and Networks in the Cloud

Learning Objectives
	1. Deploy a basic infrastructure to Google Cloud.
	2. Explore how Cloud Load Balancing functions in Google Cloud.
	3. Detail important VPC compatibilities including routing tables, firewalls and VPC peering.
	4. Identify the purpose of and use cases for Google Compute Engine.
	5. Explore the basics of networking in Google Cloud.
	6. Outline how Google Compute Engine can scale.


VPC Networking
	- secure indiv private cloud compute model hosted within public cloud
	- connect google cloud resources to internet
	- segment networks

	Main goal
		DR

Compute Engine
	- config like physical server
	- billed by second with one minute minimum
	- discound by the longer they run minimum of 25% of a month, discount every add minute

	Predetermine workloads/stable
		upto 57% discount
		min contract of 1 year or 3 years

	premptible/spot VM
		90% price
		okay for VM to be terminated
		use case
			finish batch job analysizng large data set


Scaling virtual Machines
	scale out
		horizontal scaling

	sacle up
		vertical scaling

	machine type

Important VPC compatibilities
	Firewall defined through tag
		is VM level
		Security group(VM level) and NACL(subnet level) in AWS

	VPC peering
		comm two VPC

	Shared VPC
		where you can configure who and what in one project to interact with other VPC


Cloud Load Balancing
Types
	1. Global HTTP(S)
		- cross regional load balancing web app

	2. Global SSL pRoxy
		secure socket layer traffic

	3. Global TCP proxy
		TCP wiht no SSL

	4. Regional External passthorugh netwokr load balancer	
		UDP traffic or taffic on any  port number across region

	5. Regional internal load balancer (proxy network LB/ Pasthrough network LB and Application load balancer)
		Load balance inside  bet presentation and busines layer of app

	6.Cross-region internal load balancer
		- layer 7 load balancer  traffic to backend services


Cloud DNS and CDN
	Cloud DNS
		- set url name like AWS route 53 

	Edge caching(Cloud CDN)/Edge locations in AWS

	- can be enable with External LB service

Connecting networks to Google VPC
Ways to  conenc on premise to Google cloud
	1 . Cloud VPN/ AWS VPN
		- a tunnel connection between on-premise or AWS/Azure network to GCP VPC securely over internet using 
		 	- IPsec
		 	- cloud router 
				uses border gateway protocol to exchange route information over VPN

		Limitation
			- Security concerns
			- BW reliability

	2. Peering
		- putting router in the same public data center/colocation Data center as a google PoP adn not on public internet
			Point of Presence(PoP)
				- loc where different networks can connect and share data on internet service provider infra
					- reduces latency to google cloud services and resources
		
		2.1 Direct Peering
			When to use
				- connect on premise network to google cloud network

		2.2 Carrier Peering
			When to use
				- customer not covers in google PoP connect through GCP with partner ISP

		Limitation
			- Not covered by Google Service Level Agreement
		
	3. Dedicated Interconnect
		- allows for one or more direc, private connections to google
		- if met with google specs, SLA upto 99.99%
		- connection backed up by VPN
		
		When to use
			- getting highest uptimes for interconnection
		
	4. Partner Interconnect
		- provides connectivity between onprimes and VPC network through supported ISP
		When to use
			- on premise DC can't connect to colocation DC
			- dat dont neet an entire 10 GB per second conenction
			- support critical services or app that can tolerate downtime
		
	5. Cross Cloud Interconnect
		- establish high BW dedicated connectivity bet google cloud and another cloud service provider
		- available in 10GbpS or 100Gbps

		- Cloud interconnect/AWS Direct connect
			- a private hallway comm between onpremise or google cloud
				- on premise  and google cloud connects through colocation facility using BGP bet Cloud router and onpremise router
		 

Getting started with VPC networking and GCE
Objectives
	- Explore the default VPC network
	- Create an auto mode network with firewall rules
	- Create VM instances using Compute Engine
	- Explore the connectivity for VM instances

Firewall Priority
	- higher number indicates lower priorities

Test network
	ping -c 3 <Enter mynet-eu-vm's internal IP here>
		allow-custom
	ping -c 3 <Enter mynet-eu-vm's external IP here>
		allow-icmp

Which firewall rule allows the ping to mynet-eu-vm's external IP address?
	allow-icmp

Quiz
1. In Google Cloud VPCs, what scope do subnets have?
	- Regional

2. What is the main reason customers choose Preemptible/Spot VMs?
	- To reduce cost

3. For which of these interconnect options is a Service Level Agreement available?
	- Dedicated Interconnect

4. How does Cloud Load Balancing allow you to balance HTTP-based traffic?
	- Across multiple Compute Engine regions.


Week 5 Storage in the Cloud

Learning Objectives	
	1. Deploy an application that uses Cloud SQL and Cloud Storage
	2. Distinguish between Google Cloud's Database storage options
	3. Distinguish between Cloud Storage Classes
	4. Identify the purpose of and use cases for Cloud Storage

Google Cloud storage options:

1.Cloud Storage/S3 contains
	 - Object Storage/Bucjets
	 	- manage data as an objects/binary form and not file storage or block/disk storage

	 - metadata
	 	- global unique ID
	 	- date created, author, resource type and permission
	 	- all unique keys in the form of URLs

	 Retrieval
	 	- URL via direct download
	 
	 Note
	 	- Storage objects are  immutable
	 	- select end user nearest location for minimize latency 

	 Feautures
	 	- versioning

	 Security best pracitices
	 	- IAM roles
	 	- Access Control lists(ACL)
	 		- who can access and perform an action 
	 			- can be specific to user or group of users
	 		- permission
	 			- what actions can be performed like read and write
	 	- Lifecycle Management policies
	 		- dekete objects older than 365 days

	 Use cases
	 	- Binary large-object(Blob) Storage
	 		- website content
	 		-backup and archiving
	 	- static web hosting
	 	- direct download



Cloud Storage: Storage classes and data transfer
Classes
	1. Standard Storage
		- hot data/frequently access data
		When
			- stored for only brief periods of time
	2. Nearline Storage
		- once a month/infrequetly access data
		When
			- reading/modifying data on onec a month
			- data backups
			- long term multimedia content
			- data archiving
		

	3. Coldline Storage
		- once every 90 days
		- low cost for infrequent access data

	4. Archive Storage
		- once a year
		- cheapest but has higher cost for data access
		- min 365 min storage duration
		When to use
			- online backup and DR

	5. Auto class/Intelligient Tiering
		- ideal for unknown/changin access patterns

Ways to import
	- cloud sdk
		#list bucket lists
			gcloud storage buckets list 

		#upload file in a bucket
		gsutil cp index.html gs://rayan_bucket-1

		#list files in bucket
		gsutil ls gs://rayan_bucket-1
	- google cloud flatform
		-
Storage Transfer Serices
	- import large amounts of online data into GCS quickly and cost effectively

Transfer Appliance/Snowglobe
	- data migration

Import/export tables to and from 
	- BigQuery
	- Cloud SQL

Others
	- store app engine logs
	- files for backups
	- store VM startup scripts
	- GCE images


2. Cloud SQL
	- Fully managed Relational DB
		- MySQL
		- PostgreSQL
		- SQL Server

	-supports
		- scale up
		- Master Slave auto replication
		- backupos
		- APP Engine
			- Connector/J for Java
			- MySQLdb for Python

3. Cloud Spanner
	- fullu managed Relational DB but better than cloud SQL
		- tens of thjousands reads and writes per second
	- supports
		- scale out/horizontal

4. Firestore
	- noSQL key values
	- supports
		- mobile
		- web
		- server dev
		- caches
		- auto multi region data replication
		- atomic batch operations
		- real transaction support
	- table is collection
		- documents is key value pair
	- charged at
		- each doc read/write/delete
		- doc read per query
		- ingress is free
		- 10GB free network egress per month between US region
		- Per day quota
			- 50,000 deoc reads
			- 20,000 doc deletes
			- 1 GB stored data

5. Cloud Bigtable
	- nosql big data DB

	Who 
		search
		analtyics
		maps
		gmail

	when
		- operation and analytical app 
			- IoT
			- user anlaytics
			- financial data analysis
			- one TB of semi structured/sturctured data

	Can collaborate with
		Batch processes 
			- Hadoop MapReduce
			- Dataflow
			- Spark

Comparing storage options
	Cloud Storage
		Best for
			- Storing immutable blobs larger than 10 MB

		Capacity
			PB
			Max unit size: 5 TB per object

	Cloud SQL
		Best for
			- full SQL support for an online transaction processing sys
			- web frameworks and existing app

		Capacity
			Up to 64 TB

	Spanner
		Best for
			- full SQL support for online transaction processing system
			- hor scaling/scale out

		Capacity
			PB

	Firestore
		Best for
			Massive scaling and predictability together w real time query results and offline query support
		Capacity
			TB
			Max unit size: 1MB per entity

	Cloud Big table
		Best for
			- storing large amt of structured objects
			- not support SQL queries and multi-riow transactions
			- analytical data with heavy read and write events
		Capacity
			Petabytes
			Max unit size: 10MB p/cell, 100 MB p/row

	Big Query
		- On the edge between data storage and data processing

Getting Started with Cloud Storage and Cloud SQL

Objectives
	- Create a Cloud Storage bucket and place an image into it.
	- Create a Cloud SQL instance and configure it.
	- Connect to the Cloud SQL instance from a web server.
	- Use the image in the Cloud Storage bucket on a web page.

GCE Auto start up Script
	apt-get update
	apt-get install apache2 php php-mysql -y
	service apache2 restart

GCS cloud shell
	export LOCATION=ASIA

	//Create bucket
	gcloud storage buckets create -l $LOCATION gs://$DEVSHELL_PROJECT_ID
	//Copt image to bucket
	gcloud storage cp gs://cloud-training/gcpfci/my-excellent-blog.png my-excellent-blog.png
	//Modify ACL to be readably by everyone
	gcloud storage cp gs://cloud-training/gcpfci/my-excellent-blog.png my-excellent-blog.png


qwiklabs-gcp-04-15d48010786c:us-central1:blog-db
34.27.40.243


Sample Web php MySQL connect code
<html>
<head><title>Welcome to my excellent blog</title></head>
<body>
<h1>Welcome to my excellent blog</h1>
<?php
 $dbserver = "CLOUDSQLIP";
$dbuser = "blogdbuser";
$dbpassword = "DBPASSWORD";
// In a production blog, we would not store the MySQL
// password in the document root. Instead, we would store it in a
// configuration file elsewhere on the web server VM instance.

$conn = new mysqli($dbserver, $dbuser, $dbpassword);

if (mysqli_connect_error()) {
        echo ("Database connection failed: " . mysqli_connect_error());
} else {
        echo ("Database connection succeeded.");
}
?>
</body></html>

Quiz
1. What is the correct use case for Cloud Storage?
	- Cloud Storage is well suited to providing durable and highly available object storage.

2. Which database service can scale to higher database sizes?
	- Cloud Spanner


3. Why would a customer consider the Coldline storage class?
	- To save money on storing infrequently accessed data.


Week 6 Containers in the Cloud

Learning Objectives	
	1. Define the concept of a container and identify uses for containers.
	2. Identify the purpose of and use cases for Kubernetes and Google Kubernetes Engine.

Introduction to containers
	- an isolated box around your code and its independencies
	- limited access to its own partition of the fiel sys and hardware
	- quick to start same as a process
	- only need
		- OS kernel 
	- scale like PaaS but flexible as IaaS


Kubernetes
	- open source for managing and scaling containerized applications
		- on many hosts
		- scale them as microservices
		- deployment
		- rollouts
		- roullbacks
	- bootstrapped using google kubernetes engine

	Cluster
		- a set of nodes(compute instatnce) that run containers

	Pod
		- unit that contains containers

Google Kubernetes Engine
	- google hosted managed kubenetes service in the cloud

	Advanced Cluster management feautures include:
		- Google Cloud's load balancing for compute engine instances
		- node pools to designate subsets of nodes within a cluster for additional flexibility
		- automatic scaling of your cluster's node instance count
		- automatic upgrades for your cluster's node software
		- node auto-repair to maintain node health and availability
		- logging and monitoring with Google Cloud's operation suite for visibility into your cluster.

//Start up kubernete on a cluster in GKE
	gcloud contsainer clusters create k1

Quiz
1. What is a Kubernetes pod?
	- A group of containers
		- containers in a pod are deployed/replicated as a group/together
2. Where do the resources used to build Google Kubernetes Engine clusters come from?
	- Compute Engine


Week 7 Applications in the Cloud

Learning Objectives	
	1. Describe how Cloud Functions can support application development on Google Cloud.
	2. Identify the purpose and use cases for Cloud Run.
	3. Deploy a containerized application on Cloud Run.

Cloud Run
	- a serverless PaaS that runs stateless conatiners via web req or Hub sub events
	- Built on Knative
	- auto scale out and up

	Ways/Deploy through
		1. Source based workflows
			- deploy through source code
				- builpacks
					- an open source that builds image from source code
		2. Container-based workflow
			- deploy through docker image

	Pricing
		1. handling req
			- add fee per 1 million requests
		2. startup
		3. shutdown

	Limitation
		- only available for linux amd/64 and arm/64
			- only in GKE can you deploy arm/64 

Development in the Cloud

Cloud Function
	- a serverless/function as servuce that is event-driven.
		- serverkless means no servers but you just don't worry to manage/provision them, GCP manage them.

	Deploy through
		1, Source code

	Trigger
		1. Cloud Storage
			- async exec
		2. PubSub
			- async exec
		3. HTTP invocation
			- sync exec
	
	Usecases
		1. Convert format
		2. Convert thumbnail size
		3. Store new files

Hello Cloud Run
Qwiklabs
	https://googlecoursera.qwiklabs.com/focuses/34708884?parent=lti_session

Objectives
	1. Enable the Cloud Run API.
	2. Create a simple Node.js application that can be deployed as a serverless, stateless container.
	3. Containerize your application and upload to Container Registry (now called "Artifact Registry.")
	4. Deploy a containerized application on Cloud Run.
	5. Delete unneeded images to avoid incurring extra storage charges.

SDK used command
	//gcloud cheatsheet
		https://cloud.google.com/sdk/docs/cheatsheet

	//enable cloud run api
		gcloud services enable run.googleapis.com

	//set project
		gcloud config set project

	//set region
		gcloud config set compute/region "REGION"

	//list account name
		gcloud auth list

	//list project ID
		gcloud config list project

	//one liner with two command
		mkdir helloworld && cd helloworld

	//Build container image through gcloud command and store it ing Cloud Registry
		gcloud builds submit --tag gcr.io/$GOOGLE_CLOUD_PROJECT/helloworld

	//list container images
		gcloud container images list

	//register gcloud as credential helper for all google supported docjer registry
		gcloud auth configure-docker

	//run container from cloud shell
		docker run -d -p 8080:8080 gcr.io/$GOOGLE_CLOUD_PROJECT/helloworld

	//Verify
		curl localhost:8080

	//Deploy to cloud run
		Location=asia-southeast1
		gcloud run deploy --image gcr.io/$GOOGLE_CLOUD_PROJECT/helloworld --allow-unauthenticated --region=$LOCATION

	//Delete container image
		gcloud container images delete gcr.io/$GOOGLE_CLOUD_PROJECT/helloworld

	//Delete cloud run service
		gcloud run services delete helloworld --region="REGION"


Sample Dockerfile
	# Use the official lightweight Node.js 12 image.
	# https://hub.docker.com/_/node
	FROM node:12-slim

	# Create and change to the app directory.
	WORKDIR /usr/src/app

	# Copy application dependency manifests to the container image.
	# A wildcard is used to ensure copying both package.json AND package-lock.json (when available).
	# Copying this first prevents re-running npm install on every code change.
	COPY package*.json ./

	# Install production dependencies.
	# If you add a package-lock.json, speed your build by switching to 'npm ci'.
	# RUN npm ci --only=production
	RUN npm install --only=production

	# Copy local code to the container image.
	COPY . ./

	# Run the web service on container startup.
	CMD [ "npm", "start" ]

Quiz
1. Select the managed compute platform that lets you run stateless containers through web requests or Pub/Sub events.
	- Cloud run

2. Cloud Run can only pull images from (Select 2): 
	- Artifact Registry
	- Docker Hub

3. Why might a Google Cloud customer choose to use Cloud Functions?
	- app contains event-driven code that they don't want to provision compute resourcees for.


Week 8 Course Summary

Learning Objectives	
	1. Review learning concepts from throughout the course.

Course Summary
Module 1 - Introducing to Google Cloud
	- Google Cloud and Cloud Computing
	- Managed Infrastructure and services with IaaS and PaaS
	- The Google Cloud network
	- Security throughout the infrastructure
	- Pricing structure and billings tools

Module 2 - Resources and Access in the Cloud
	- Google Cloud Resource Hierarchy
		- Org node
			- Folders
				- Projects
					- Resources
	- Policies
	- Identity and Access Management(IAM)
	- Ways to access and interact with Google Cloud
		- Google Cloud console
		- Cloud SDK and Cloud Shell
		- APIs
		- Google Cloud App

Module 3 - VMs and Networks in the Cloud
	- Google Compute Engine(GCE)
	- Virtual Private Cloud(VPC)
	- Compute Engine's Autoscaling feature
	- Google Virtual Private Cloud compatibility features
		- Routing tables
		- Firewalls
		- VPC peering
		- Shared VPC
	- Cloud Load Balancing
	- Google Cloud Interconnect

Module 4 - Storage in the Cloud
	- Google Cloud's core storage options
		- Cloud Storage
		- Cloud Bigtable
		- Cloud SQL
		- Cloud Spanner
		- Firestore
	- Four Cloud Storage classes
		- Standard
		- Nearline
		- Coldline
		- Archive

Module 5 - Containers in the Cloud
	- Containers
	- Kubernetes
	- Google Kubernetes Engine(GKE)

Module 6 - Applications in the Cloud
	- Cloud Run
	- Cloud Functions

Course Resources links
	https://d3c33hcgiwev3.cloudfront.net/-YEvkqp0Tk-ob7J6Uzzj_w_23424d3021424220856ea3ae03e9f5a1_T-GCPFCI-B-_-Course-Resources-_-Google-Cloud-Fundamentals_-Core-Infrastructure.pdf?Expires=1710374400&Signature=MCrrmJq5Nx9aHIrDS7-3EzaSTyIdjKPBveL6-Gw0B-WB4T1qoPnamIp4a3HCc0cBGMGD2oQB9PhlxtL3l7g~on4ocQIVJdoqdikIJn6llf7Zmr-9I5-jbQORoXcQhvcRkjsiqgukNP1Rh0HXejVZL0upmPeHrMod61yURguZutM_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A

Essential Google Cloud Infrastructure: Foundation

Layman's Term of the following:
	Cloud Infrastructure
		Imagine you're in a City. 
			- The infrastructure is the basic facilities and sys such as electricity, communications, water, fuel, sewerage systems. 
			- The people are the end users 
			- The cars, bikes, motorcycle and buildings like a mall/restaurant are the applications

	Container 
		- portable box which contains an app

	Kubernetes 
		- ochestrate code in containers

Module 1: Interacting with Google Cloud

	Learning Objectives 
		1. List the different ways of interacting with Google Cloud 
		2. Interact with the Google Cloud console and Cloud Shell 
		3. Create Cloud Storage buckets 
		4. Deploy solutions using Google Cloud Marketplace

	Using Google Cloud
		Ways to Interact with Google Cloud 
			1. Google Cloud console - Web user interface

			2. Cloud Shell and Google Cloud CLI/SDK/gcloud
				- CLI

				Cloud shell
					- Temporary Compute Engine VM
					- Command-line access to the instance via a browser
					- 5 GB of persistent disk storage ($HOME dir)
					- Pre-installed Cloud SDK and other tools
					- gcloud: for working with Compute Engine and many Google Cloud services
					- gcloud storage: for working with Cloud Storage
					- kubectl: for working with Google Kubernetes Engine and Kubernetes
					- bq: for working with BigQuery
					- Language support for Java, Go, Python, Node.js, PHP, and Ruby
					- Web preview functionality
					- Built-in authorization for access to resources and instances

			3. Rest-based API
				- For custom applications

				Restful API
					- get/post/put/delete
					- JSON

				Two pupose why google cloud client Librariers exposes APIs
					App APIs
						- provide access to services

					Admin APIs
						- functionality for resource management
						- build auttomated tools

			4. Cloud Mobile App
				- For iOS and Android

				Features:
					- manage VM and DB instances
					- manage apps in app engine
					- manage your billing
					- Visualize projects w/ customizable dashboard

			Qwiklabs - Working with Google Cloud Console and Cloud Shell

				Objectives:
					- Get access to Google Cloud
					- Create a GCS using the google cloud console
					- Create a GCS bucket using cloud shell
					- Become familiar with Cloud shell features

				Create a GCS bucket using cloud shell
					//Create GCS bucket
						gcloud storage buckets create gs://[BUCKET_NAME]

					//upload my local file to GCS bucket
						gcloud storage cp [MY_FILE] gs://[BUCKET_NAME]

						//if file name has spaces
							gcloud storage cp â€˜my file.txt' gs://[BUCKET_NAME]

				Create a persistent state in Cloud Shell
					//list current config
						gcloud config list

					//config set project
						gcloud config set project

					//list all available regions
						gcloud compute regions list

					config file
						REGION=asia-southeast1
						PROJ_ID=realve-study

					.profile
						source config

					To create a persistent state in Cloud Shell, which file would you configure?
						-.profile

			Qwiklabs - Infrastructure Preview

				Objectives
					- Use Marketplace to build a Jenkins Continuous Integration environment.
					- Verify that you can manage the service from the Jenkins UI.
					- Administer the service from the Virtual Machine host through SSH.

				Google Cloud Marketplace lets you quickly deploy functional software packages by providing pre-defined templates with which Google Cloud service?
					- Deployment Manager
						- acquiring a virtual machine instance and installing and configuring software for you.
						- Google Cloud service that uses templates written in a combination of YAML, python, and Jinja2 to automate the allocation of Google Cloud resources and perform setup tasks
						Behind the scenes created
							- VM
								- startup script to install and config software
								- network FW rules to allow traffic to the sercvice

					Shut down and restart Jenkins service
						sudo /opt/bitnami/ctlscript.sh stop
						sudo /opt/bitnami/ctlscript.sh restart

	Quiz

	1. Which of the following does not allow you to interact with Google Cloud?
		- Cloud Explorer

	2. What is the difference between the Google Cloud Console and Cloud Shell?
		- Cloud Shell is a command-line tool, while the Cloud Console is a graphical user interface

Module 2: Virtual Networks

	Learning Objectives
		1. List the VPC objects in Google Cloud 
		2. Explore VPC Networking. 
		3. Implement Private Google Access and Cloud NAT

	VPC 
		- an isolated/private network where you can define IP address range to be used for your resources

	Projects, Networks and Subnetworks

		Projects
			- associates objects and services with billing
			- 15 networks

		Networks
			- can be shared with outher projects or be peered with VPC peering

			CIDR(classless inter-domain routing)
				- ways of splitting network to a smaller network called subnet
				- method for allocating IP addr

		Types
			Default
				- every project
				- one subnet per region
				- default firewall rules

				Auto Mode
					- Default network
					- one subnet per region
					- regional IP allocation
					- fixed /20 subnet per region
					- expandable upto /16

			Custom Mode
				- no default subnets created
				- full control of IP ranges
				- Regional IP allocation
				- Expandable to IP ranges

			Subnetworks
				- smaller network from splitted network
				- works on regional level
				- cross zone is possible
					- single FW rule can be applied to both VMs
				- 4 reserves IP addrs compares to AW which has 5
					- first
						- network addr
					- second
						- subnet's gateway
					- last
						- broadcast addr
					- ip before last
						- - broadcast addr

				-subnet mask
					- allows part of undelying IP to get add next values from next base IP

					Rules
						- cannot overlap with other subnets
						- ip range must be unique valid CIDR block
						- new subnet IP ranges have to all within valid IP ranges
						- can expand but not shrink
						- auto mode can be expanded from /20 to /16

			Notes
				- Same VMs located in same network can communicate using private addr
				- Two VMs in different network can comm with their private addr if using Google Edge routers/VPN
					- diff bill and security risks

			Expand Subnet
				- change sider notation to lesser number

				/29
					4 instances

				/24
					256-4= 252 instances

				/23
					508 instances

	IP Addresses
		Internal IP 
			- allocated from subnet range to VMs by DHCP
		
		DHCP(Dynamic Host Configuration Protocol) 
			- allows hosts to automatically learn various netwokr w/o manual static config 
			- automatically learn address of DNS server 
			- DHCP lease is renewed every 24 hrs 
			- VM name + IP is registered with network-scoped DNS 
			
		IPV4 ranges
			10.0.0.0/8
			10.0.0.0 to 10.255.255.255
			172.16.0.0/12
			172.16.0.0 to 172.31.255.255
			192.168.0.0/16
			192.168.0.0 to 192.168.255.255

			External IP
				- assigned from pool(ephemeral/temporary)
				- reserved(static)
				- bring your own IP address(BYOIP)
				- VM doesn't know external IP
					- It is mapped to the internal IP

			Quotas/limits of instance per network
				- 15,000 VM

	Maopping IP addresses
		External IPs are mapped to internal IP

		Two types of internal DNS
			- Zonal(preferred)
			- Global

		Note
			- DNS name always poins to specific instance no matterr what internal IP addr is.

		Cloud DNS
			- google's DNS service
			- 100& uptome SLA
			- create and update millions of DNS records

		Alias IP range
			When to use
				- multiple service running on a VM and want assigh diff IP addr to each service

	Routes and Firewall Rules
		Cloud Routes \
		- defines the paths that network traffic takes from a virtual machine (VM) instance to other destination(same VPC network or outside).

		GCP Firewall Rules/Security group
			- stateful that allow/deny connections to or from your VM instances
			- Ipmlied deny all ingress and allow all egress by default
			- applited tot he network as a whole

			Use case(Egress)
				- Going out
				- VM to internet
				- VM to VM

				Conditions
					- Desination CIDR ranges
					- Protocols
					- Ports

				Action:
					Allow
						- permit the matching egress connection

					Deny
						- block the matching egress connection

			Use case(Egress)
				- Going in
				- internet to VM
				- VM to VM

				Conditions
					- Source CIDR ranges
					- Protocols
					- Ports

				Action:
					Allow
						- permit the matching egress connection

					Deny
						- block the matching egress connection

	Pricing
		Network Pricing
			No charge 
				- Ingress 
				- egress to same zone(internal IP) 
				- egress to google products like maps/drive/youtube 
				- egress to dif cloud service within same region 
				- Staic and ephemeral external IP addr atttached to forwarding rules used by Cloud NAT/VPN tunnel

			Charge
				- egress to same zome(External IP per GB)
				- egress bet regions within US and CANADA(per GB)
				- egress between zones in the same region(per GB)
				- static external IP addr assigned but unused
				- static and ephemeral external IP addr used on standard VM instances
				- static and ephemeral IP add in use on preemptible VM instance

			Note
				Use Google CLoud Pricing Calculator

	Qwiklabs - VPC Networking
		Link
			https://googlecoursera.qwiklabs.com/focuses/34733566?parent=lti_session
		
		Objectives
			In this lab, you learn how to perform the following tasks:

			1. Explore the default VPC network
			2. Create an auto mode network with firewall rules
			3. Convert an auto mode network to a custom mode network
			4. Create custom mode VPC networks with firewall rules
			5. Create VM instances using Compute Engine
			6. Explore the connectivity for VM instances across VPC networks

		Without a VPC network, you cannot create VM instances, containers, or App Engine applications.
			- True

		Which firewall rule allows the ping to mynet-eu-vm`s external IP address?
			- allow-icmp

		Create custom VPC network and subnet
			//gcloud command
				gcloud compute networks create managementnet --project=qwiklabs-gcp-01-b53880c2c02c --subnet-mode=custom --mtu=1460 --bgp-routing-mode=regional

			//create subnet
				gcloud compute networks subnets create managementsubnet-us --project=qwiklabs-gcp-01-b53880c2c02c --range=10.240.0.0/20 --stack-type=IPV4_ONLY --network=managementnet --region=us-east4

		//Create custom network
			gcloud compute networks create privatenet --subnet-mode=custom

		//Create subnet-us
			gcloud compute networks subnets create privatesubnet-us --network=privatenet --region=us-east4 --range=172.16.0.0/24

		// create the privatesubnet-eu subnet
			gcloud compute networks subnets create privatesubnet-eu --network=privatenet --region=europe-west4 --range=172.20.0.0/20

		//list all available VPC networks
			gcloud compute networks list

		//list all available VPC subnet
			gcloud compute networks subnets list --sort-by=NETWORK

		//create FW rule for privatenet-allow-icmp-ssh-rdp
			gcloud compute firewall-rules create privatenet-allow-icmp-ssh-rdp --direction=INGRESS --priority=1000 --network=privatenet --action=ALLOW --rules=icmp,tcp:22,tcp:3389 --source-ranges=0.0.0.0/0

		// list all FW rules
			gcloud compute firewall-rules list --sort-by=NETWORK

		//gcloud create instance
			gcloud compute instances create managementnet-us-vm \
				--project=qwiklabs-gcp-01-b53880c2c02c \
				--zone=us-east4-b \
				--machine-type=e2-micro \
				--network-interface=network-tier=PREMIUM,stack-type=IPV4_ONLY,subnet=managementsubnet-us \
				--metadata=enable-oslogin=true \
				--maintenance-policy=MIGRATE \
				--provisioning-model=STANDARD \
				--service-account=109867070428-compute@developer.gserviceaccount.com \
				--scopes=https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring.write,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/trace.append \
				--create-disk=auto-delete=yes,boot=yes,device-name=managementnet-us-vm,image=projects/debian-cloud/global/images/debian-11-bullseye-v20240213,mode=rw,size=10,type=projects/qwiklabs-gcp-01-b53880c2c02c/zones/us-east4-b/diskTypes/pd-balanced \
				--no-shielded-secure-boot \
				--shielded-vtpm \
				--shielded-integrity-monitoring \
				--labels=goog-ec-src=vm_add-gcloud \
				--reservation-affinity=any
				
		//gcloud create instance tho privatenet-us-vm
			gcloud compute instances create privatenet-us-vm --zone=us-east4-b --machine-type=e2-micro --subnet=privatesubnet-us --image-family=debian-11 --image-project=debian-cloud --boot-disk-size=10GB --boot-disk-type=pd-standard --boot-disk-device-name=privatenet-us-vm
		
		//list all VM instances
			gcloud compute instances list --sort-by=ZONE

		Which instances should you be able to ping from mynet-us-vm using internal IP addresses?
			- mynet-eu-vm
				- same VPC network called mynet eventhough seprate zones,regions and continents can communicate

		Review
			- VPC networks are by default isolated private networking domains. 
			Therefore, no internal IP address communication is allowed between networks, 
			unless you set up mechanisms such as VPC peering or VPN.

Common Network Designs
	1. Availability and Disaster Recovery
		- Increased availibility with multiple zones
			- Two Vm instances in one network, subnet and region but different zone
		
		- Globalization with multiple regions
			- Two VM in same network, diff subnet, region and zone

	2. Security
		- Cloud NAT provides internet access to private instances
			Scenario
				One region
					- 2 Instances in different subnet
					- only use Internal IP and no Exteranl IP to make it private
					- use cloud NAT for egress access internet such as update/patching/config management server
						Cloud NAT	
							- managed network address translation service
							- does not allow ingress traffic
							- Keep VPC isolated and secured
		
		- Private Google Access to Google APIs and services
			- uses VPC Routing to allow communication between VM in different region in the same network
			- enable private Google access to allow VM instance  to reach external IP addr of Google APIs and services
				- Private VM instance need access to GCS bucket
				- Private Google Access
					- subnet level
					- only internal IP and no effect on external IP

	Qwiklabs - Implement Private Google Access and Cloud NAT
	https://googlecoursera.qwiklabs.com/focuses/34735427?parent=lti_session
	
	Objectives
		In this lab, you learn how to perform the following tasks:
			1. Configure a VM instance that does not have an external IP address
			2. Connect to a VM instance using an Identity-Aware Proxy (IAP) tunnel
			3. Enable Private Google Access on a subnet
			4. Configure a Cloud NAT gateway
			5. Verify access to public IP addresses of Google APIs and services and other connections to the internet

		VM Instance creation
			- VM instance have ephemeral external IP bu default
				- can be cahnge in policy constraint at org/project level

		// SSH connectivity gcloud
			gcloud compute ssh vm-internal --zone us-east1-d --tunnel-through-iap

		Note:
			No external IP addr	
				- an only be reached by other instances on the network via a managed VPN gateway or via a Cloud IAP tunnel
			Cloud IAP 
				- enables context-aware access to VMs via SSH and RDP without bastion hosts

			Currently, which of your VM instances can access the image from your bucket?
				- Cloud Shell through SSH

			If Private Google Access is ON on subnet level
				- VM with only internal IP can only update for Google Ckloud packages or Google APIs and services

			Cloud NAT gateway	
				- implements outbound NAT, but not inbound NAT. 
				- in other words, hosts outside of your VPC network can only respond to connections initiated by your instances; 
				they cannot initiate their own, new connections to your instances via NAT.

			Cloud NAT logging
				- allows you to log NAT connections and errors. When Cloud NAT logging is enabled, one log entry can be generated for each of the following scenarios:
					- When a network connection using NAT is created.
					- When a packet is dropped because no port was available for NAT.
				
			For  Configure and view logs with Cloud NAT Logging pls refer to the qwiklab link above

	Quiz
	1. In Google Cloud, what is the minimum number of I addresses that a VM instance needs?
		- One: Only an internal IP address
	2. What are the three types of networks offered in Google Cloud?
		- Default network, auto network, and custom network.
	3. What is one benefit of applying firewall rules by tag rather than by address?
		- When a VM is created with a matching tag, the firewall rules apply irrespective of the IP address it is assigned.
			- ephemeral external IP addr is random that is why we use tag to make it simple and assigning match FW rules


Module 3: Virtual Machines

	Learning Objectives
		1. Recall the CPU and memory options for virtual machines
		2. Describe the disk options for virtual machines
		3. Explain VM pricing and discounts
		4. Create and customize VM instances using Compute Engine

	Compute Engine
		- are a collection of networked services which includes persistent disks that are network-attached. 
		In some cases the Google Cloud VM behaves unlike hardware or other kinds of virtual machines, 
		for example, when a multi-tenant virtual CPU "bursts", using excess capacity beyond the VM spec.
	
		Hardware Limitations
			- CPU 
			- GPU
			Fixed
				- TPU(Tensor Processing Unit)
					- used in AI and training ML
		
		Machine type	
			- Network throughput scales 2 Gbps per vCPU
				- C3 series
					- max of 200 Gbps with 176 vCPU

		Storage Disks
			- standard
			- SSD
				- higher IOPS than standard
			- Local SSD
				- create a ramdisk
				- swap disk
				- non persistent
				- higher throughput and lower latency thatn SSD
				
		Networks
			- one network and subnet for internal IP is required but external IP is optional
	
	VM access and lifecycle
		Access
			Linux
				- SSH
				- tcpL22

			Windows
				- windows
				- tcp:3389

		Lifecycle
			provisioning	
				Staging
						running
							instances.Suspend()
								suspending
									suspend
										instaces.resume()
											go back to provisioning
							repairing
								instances.stop()
									stopping
										terminated
											instances.start()
												go back to provisioning
											instances.delete()
												deleted
			Shutdown script
				~ 90s
		
		OS patch management	
			- use to OS patches across a set GCE VM instances

		Price
			Charged
				- Attached Disks
				- Reserved IP addresses
			Not Charged	
				- Memory 
				- CPU resources

		Actions supported on a Terminal VM
			allowed	
				change machine type
				migrate VM instance to another network
				add/remove attached disks, change auto delete settings
				modify instance tags
				modify cusomt VM or project-wode metadata
				remove or set a new static IP
				modify VM availability policy
			Not allowed
				- cannot change the image of a stopped VM
	
		Actions allowed or running VM
			allowed
				add network tag
				add disks
				det if boot disk is deleted when instance is deleted
					deleted by default
				detached boot disk
					create image
			Not allowed	
				change
					machine type
					CPU platform
					zone
					convert non-preemptibleto preemptible
						preemptible can be interrupted
		Note:
			- VM policies can be change to allow thus actions above while VM is running.
	
		//Inside VM
		//see info about unused, used memory and swap space
			free
		//see RAM details
			sudo dmidecode -t 17
		//see number of processors
			nproc
		//see details of CPU
	
	Qwiklab - Working with Virtual Machines
		Objectives
			In this lab, you learn how to perform the following tasks:

			1. Create several standard VMs
			2. Create advanced VMs

	Compute options
		https://cloud.google.com/compute/docs/machine-resource

		Machine type structure	
			Machine Family -> machine series -> machine type
		Four Compute Engine Machine Families
			1. General-purpose
				- used for standard and cloud native workloads
				E2 machine series
					- 2 to 32 vCPUS with ratio of 0.5GB-8GB of memory per vCPU
					- no app dependencies on specific cpu architecture
						- web servers
						- app server
						- back office apps
						- small to medium DB
						- microservices
						- virtual desktops
						- development and test environments
					- shared-core
						- 0.25-1 vCPUs w/ 0.5-8GB of memory
				N2,N2D, N1
					- most flexible VM 
					When to use
						- web servers
						- app server
						- back office apps
						- medium to large DB
						- cache
						- media/streaming
					- uses Intel CPU	
						- 128 vCPUs and 0.5 to 7HN RAM per vCPU
							- Cascade Lake CPU
								- default CPU upto 80 vCPUs
							- Ice Lake
								- default CPu for >80 vCPUs
					- EPYC milan and EPYC Rome processors
						- upto 224 vCPUs per node

				Tau T2d and Tau T2a VMs
					- uses 3rd gen AMD EPYCTM processors
					- scale out optimized

					When to use
						- scale out workloads
						- web server
						- containerized microservices
						- media transcoding
						- large-scale java applications
						
					T2d
						- upto 60 vCpUs per Vm with 4GB RAM per vCPU
					
					T2a
						- run on ARM processors
							x64 Ampere Altra Processor with ARM instruction with 3GHz

						- use GKE fo cost optimized

			2. Compute-optimized
				C2
					When to use
						- compute-bound workloads
						- high-performance web server
						- gaming(AAA game servers)
						- Ad server
						- High performance computing (HPC)
						- Media transcoding
						- AI/ML

				C2D
					When to use
						- memory-bound workloads
						- gaming(AAA game servers)
						- High performance computing (HPC)
						- High performance DB
						- Electronic Design Automation(ADA)
						- Media transcoding

				H3
					- High performance computing (HPC)
					- Electronic Design Automation(ADA)

			3. Memory-optimized
				M1
					When to use
						- medium in-memory DB such as SAP HANA
						- tasks that req intensive use of memory w/ higher memory to vCPU ratios than
						general purpose high memory machine types
						- in memory DB and in-memory analytics, business warehousing(BW) workloads,
						genomic analysis, SQL analysis services.
						- Microsoft SQL server amd similar DB
				
				M2
					When to use
						- large inmemory DB such as SAP HANA
						- in memory DB and in-memory analytics, business warehousing(BW) workloads,
						genomic analysis, SQL analysis services, etc.

				M3
					- OLAP and OLTP SAP workloads
					- memory intense electronic design Automation

			4. Accelerator-optimized
				- optimized for HPC workloads

				A2
					When to use
						- CUDA-enabled ML training and inference
						- HPC
						- Massive parallelized computation

				G2
					When to use
						- video transcoding
						- remote visualtzaiont workstation

		Creating custom machine types
			When to use	
				- requirements doesn't fit between predefine types
				- need more RAM or CPU

	Compute pricing
		- Per-second billing, w/ minimum of 1 minute means you will billed 1 minute even if your VM run for 30 sec/less than a minute
			- vCPUs
			- GPUs
			- GB of Memory
		- Resource-based pricing
			- each vCPU and GB of memory is billed separately
		- Discounts:
			- Sustained use
				- activated when VM run 25% of a month
				- automatic discounts you get for running speific resources for a significant portion of the billing month
				- 30% discount so create first day of the month cuz it reset at the beginning of each month
				- discount every incremental minute you use that instace		
				- N1(upto 30% net discount)
				- N2/N2D(upto 20% discount)	
			- Committed use
				- 1 or 3 years
				- upto 57% discounts for most machine types including custom one
				- 70% discount for memory optimized machine types
				When to use
					- workload is stable and predictable
			- Preemptible/Spot VM instance
				- might terminate/preempt these instance if google requires to access those resources
					- cuz they are excess capacity so availability varies w/ usage
				- preemptible can run max of 24 hrs at a time but Spot do not have max run time
		- Recommendation Engine
			- Notifies you of underutilized instances
		- Free usage limits

		Note: External IP incur a small cost

	Special compute configurations
		Preemptible	VM
			- max 24 hrs continuous live
			- 30 sec notif before machine slows down
			- no auto restart
				- fixs is load balancers to monitor and keep restarting preemptible VM
			- no live migrations

			Usage	
				- running batch processing jobs

			Updated version
				Spot VMs
					- finite CE resources so not always available
				
		Sole tenants node
			- physically isolate workloads

			Usage
				- physical isolation from other workloads or VM in order to meet compliance requirements
				- all VM in the same project

		Shielded VM
			- offer verifiable integrity to your VM instance
				- boot/kernel level malware/rootkits protection
			- secure boot
			- virtual trusted platform module(vTPM) shield/sealing
				- prevent data exfilitration
			- Integrity monitoring

			How to use
				- select shielded image
			
		Confidential VMs
			- encrypt data while it's being processed.
			- easy to use w/ no changes to code or performance compromise.
			- N2D Compute Engine VM running on second generation AMD Epyc(ROME) processors
				- uses AMD Secure Encrypted Virtualization(SED)
				- Confidential Computing support
				Usage	
					- compute heavy workloads
					- high memory capacity
					- high throughput
					- support for parallel workloads
			Note
				- google doesn't have access to encryption keys

			How to use
				- select Confidential VM service
					- console
					- API
					- gcloud CLi
		
	Images
		Public base images
			- Google
			- third party vendors
			- premium images(p)
			- Linux
			- windows

		Custom images
			- create new image from VM: pre-configured and installed SW
			- import from:
				- on-premium
				- workstation
				- another cloud
			- features	
				- image sharing/family/deprecation

		Machine Images
			- stores all:
					- config
					- metadata
					- permission
					- data from one or more disks
			- uses mainly in maintenance scenario such as instance cloning/replication/disk backups/recovery 

		Note:
			- Premium images(p) are charges per second after a 1 minute mon except SQL server images(charged after 10 min)

	Disk options
		Boot disk
			- VM comes w/ single root persistent disk from base image
				- avoid delete
					- disable delete boot disk when instance is deleted

		Persistent disks
			- network storage appearing as a block device
			- attached to VM through the network interface and not physically attached to the machine
			- can survice VM terminate
			- bootable means attach to VM and boot from it
			- spnapshots
			- scales with size
			- HDD or SSD options
			- disk rezising even running and attached
			- can be attached in read only mode
			- Zonal
				- reliable block storage
			- regional
				Usecase
				High availability/perfomance DB
					- active active across two zones
						- sync replication across zones
			- types
				1. pd-standard Disks
					- uses HDD
					- large data processing workloads uses sequential I/Os
					- large capacity only needed
				2. pd-SSD
					- perfomance SSD
					- enterprise app and high performance DB that req latency and more IOPS than pd-standard
				3. pd-balanced
						- uses SSD
						- max IOPS nad lower IOpS per GB
						- genearal purpose app bet pd-standard and pd-SSD
				4. pd-extreme
						- uses SSD
						- zonal only
						- highend DB workloads
						- high perforamnce for:
							- random access workloads an
							- bulk throughput
			Encrption keys
				- Google-managed
				- Cusotmer-managed
					- use CLoud key Management service
				- Customer-supplied
					- Customer-supplied encryption keys

		Local SSD
			- very high IOPS, lower latency, and higher througput than persistent disk
				- but ephemeral
			- physically attached to a VM
			- 375 GBdisk upto 24, totla of 9TB per instance
			- Data services reset but not VM stop or terminate	
			- w/o data redundancy
				
		RAM disk
			- tmfs
				- store data in memory 
			- faster than local disk but slower than memory	
				- use when app expects file sys structure and cannot directly store its data in memory
			- very volatile means erase on stop or restart
				- ephemeral
			- may need larger machine type if RAM was sized for the app
			- consider using persistent disk to back up RAM disk data

			Recommendation
				- use tmfs for fastest performance for small data structures
				- use for high memory VM along with persistent disk to back up RAM disk data

		Limitation
			Shared core machine type
				 - can only attach upto 16 disks
			standard/high memory/high CPU/memory-optimized/Compute-optimized
					- 128

		Note
			- Throughput is limited by number of compress
				- shares same BW w/ Disk IOPS
				- compete with any network egress/ingress throughput

			- Compute Hardware Disk
				- grow size
					- repartition
				- changes	
					- reformat
				- redundancy	
					- create redundant disk array
				- encryption
					- encrypt fiels before writing them to the disk
			
			- Cloud Persistent disks
				- simply grow disks and resize
					- virtual networked devices
				- redundancy
					- Snapshot
				- encryption
					- auto encrypted
				- Confidential
					- we can use own keys so no one can get data except us
				

			



	Common Compute Engine actions
		metadata	
			- useful for startup and shutdown scripts
				- use metadata server w/o additional authorization to get unique infor

		Moving instance within same region
			- automate the move by using gcloud command
				- gcloud comput instances move
			- update references to VM cuz it's not automatic
				- target VMs
				- target pools
			- process
				- shutdown
				- move to dest zone
				- restart

		Moving instance to diff region
			- process	
				- snapshot all persistent disk
					- custom image
				- create disk from that image in the new region
				- create new VM from that region
				- attach that snapshot
				- assign static external IP
				-  update references to new VM
				- delete previous VM/disks/snapshot
		
		Snapshot
			- stored in GCS
				- not visible in your buckets cuz manage by the snapshot service
			- spnapshots are incremental and automatically compress 
				- means next snapshot will only copy the changes after the first snapshot
			
			Usage	
				- backup critical data for availability and drive
				- improve disk performance such us from HDD to SSD
					- resizing disk to improve I/O performance 
			
			Note
				- not applicable to local SSD
				- can grow disks in size but can never shrink them
				- snapshot doesn't backup VM metadata, tags and et.

		Recommendation
			- store startip/shutdown scripts in GCS
		

	Qwiklab - Working with Virtual Machines
		https://googlecoursera.qwiklabs.com/focuses/34737878?parent=lti_session

		Objectives
			In this lab, you learn how to perform the following tasks:

			1. Customize an application server
			2. Install and configure necessary software
			3. Configure network access
			4. Schedule regular backups

		//make script file executable
			sudo chmod 755 /home/minecraft/backup.sh

		//Run backup script in the background
			. /home/minecraft/backup.sh

		//script sample to backup to GCS
			#!/bin/bash
			screen -r mcs -X stuff '/save-all\n/save-off\n'
			/usr/bin/gcloud storage cp -R ${BASH_SOURCE%/*}/world gs://${YOUR_BUCKET_NAME}-minecraft-backup/$(date "+%Y%m%d-%H%M%S")-world
			screen -r mcs -X stuff '/save-on\n'

		//instructs cron to run backups every 4 hours.
			0 */4 * * * /home/minecraft/backup.sh

	Quiz
	1. Which statement is true of Virtual Machine Instances in Compute Engine?
		- In Compute Engine, a VM is a networked service that simulates the features of a computer.
	2. What are sustained use discounts?
		- Automatic discounts that you get for running specific Compute Engine resources for a significant portion of the billing month
	3. Which statement is true of persistent disks?
		- Persistent disks are encrypted by default.

Essential Google Cloud Infrastructure: Core Services

Learning Objectives
	1. Understand how to navigate the course
	2. Understand how to download course resources

Module 1 - Identity and Access Management(IAM)
	- Administer Identity and Access Management for resources

	Learning Objectives
		1. Describe the IAM resource hierarchy
		2. Explain the different types of IAM roles
		3. Recall the different types of IAM members
		4. Implement access control for resources using IAM

	Identity and Access Management
		- identiyfying who can do what on which resources
		who
			- IAM member
		what
			- roles
				- permissions
		resources
			- Google cloud service

	

	Organization
		- root node for google cloud hierarchy
		- when workspace domain/identity accoun creates project, their domain name will be the name of their organization
		Roles
			Workspace/Cloud Identity super admin
				- assign the organization admin role to some users
				- be the point of contact in case of recovery issues
				- control the lifecycle of the workspace or cloud identity accound and organization resource
			- organization admin
				- control over all cloud resources
				- useful for auditing
				- define IAM policies
				- determine structure of the resource hierarchy
				- delegate responsibility over critical components such as 
					- networking
					- billing
					- resource hierarchy through IAM roles
			- project creator
				- controls project creation
				- control over who can create projects
	Folders
		- where you can isolate department -> teams -> products  or shared Infrastructure
		- allow delegation of adnmin rights

	projects
		- enabled: 
			- services, 
			- api, 
			- billing, 
			- add/remove collaborator
	
	Resource Manager Roles
		Organization
			- admin
				- full control over all resources
			- view access to all resources

		Folders
			- admin
				- full control over folders
			- creator
				- browse hierarchy and create folders
			- viewer
				- view folders and projects below a resource
		projects
			creator
				- create new projects(automatic owner) and migrate new projects into organization
			deleter
				- delete projects
	roles
		- offers fixed level access
			- owner
				- full admin access
				- invite members
				- remove members
				- delete projects
			- editor
				- deploy app
				- modify code
				- config services 
			- viewer
				- read only acess
			- billing admin
				- manage billing
				- add and remove admin
				- no rights change resources in the project
		Three types	
			1. Basic
				- owner
				- editor
				- viewer 
			2. Predefined
				- apply to aparticular google cloud service
				why	
					- grouping permissions into a role makes it easier to manage

				Sample
					Compute Engine
						- Compute admin role	
							- full control of all compute engine resources
						- Network admin role	
							- permission to create/modify/delete network resources
							- only read access to FW ruless SSL cert
							- read access to ephemeral IP addr too
					Cloud storage	
						- storage admin role
							- create/modify/delete disks/images/snapshots
							- given to manages project images without editing having edior role
			3. Custom
				When to use
					- Implementing least privileged model

	members
		- define who cand do what on wchich resources

	Types
		1. Google account
			- developer/admin/person who interacts with google cloud

		2. Service account	
			- an account that belongs to your app instead of an indiv end user
		3. Google group
			- name collection of google accounts and servic accouns
		4. Workspace domain
			- rep org internet domain
			- rep virtual group of all google accounts that have been created in an org workspace acct

		5. Cloud IAP
				- let users who don't have workspace domain manage users and groups using google admin console like wokspace domain

	IAM policies
		- policy consists of list of bindings
		- bindin bind a list of members to a role
			- role
				- named list of permissions defined by IAM 
	
	IAM allow policies
		- grant access to cloud resources
		- control access to the resource itself, as well as any descendants of that resource
		- associates, binds, one or more prinicpials(also known as a member or identity) w/ single IAM role

	IAM deny policies
		- deny rules prevent certain principals from using certain permissions, regardless of the roles they're granted
		- deny policies are made up of deny rules. Each deny rule spcifies:
			- a set of principals that are denied permissions
			- permissions that the principals are denied/unable to use
			- optional: condition that must be true for the permission to be denied
		- when principal is denied a permission, they can't do anything that requires that permission
	
	IAM Conditions	
		- enforce conditional, attribute-based access control for google resources
			- grant resource access to identities(members) only if config conditions are met
			- specified in the role bindings of a resource's IAM policy

	Organizaiton policies
		- config of Restrictions
		- defined by configuring a constraint w/ desired Restrictions
		- applied to the organization node, folders or projects
		
	Corporate directory migrations
		Microsft AD or LDAP -> Google Cloud Directory Sync -> Users and groups in your Cloud Identity domain
		
	Single sign-on(SSO)
		- use cloud identity to config SAML SSO
		- if SAML2 isn't supported, use a third party solution(ADFS, Ping, or OKTA)

	Note:
		- cannot use IAM to create or manage your users/groups
		- less restrictive parent policy will always ovverride more restrictive polociy
		- chose smallest scope
		- use role Recommendation to lessen excess permission
		- deny policies first before allow policies

	Service Accounts
		- an account that belongs to your cloud resources instead of an indiv end user
			- assistant robot that has secure access resources w/o using personal user credentials
		- identity for carrying out service to service interactions

		Types: 
			1. user-created(custom)
			2. built-in	
				- compute engine default service accounts
				- app engine default service accounts
			3. Google APIs service account
				- run internal google processes on your behalf

		When to use
			- convenient when you're not accessing user data
			- provisioning resources with terraform
			- App that interacrts with Google Cloud Storage 
	
		scopes
			- used to determine whether an authenticated identity is authorized
			- can be changed after an instance is created

		How it works?
			- programs running w/in compute engine instances can automatically acquire access tokens w/ credentials
				- tokens are used to access any service API in your project and any other services that granted access to that service account
		
		Service account permissions
			- default service accounts
				- basic roles
				- predefined roles
			- user created service accounts
				- predefined roles
			- roles for service accounts can be assigned ot gorup or users

		How service accounts authenticated?
		Two types of service account keys	
			1. Google-managed service accounts
				- all service accounts have google-managed keys
				- google stores both the public and private portion of the key
				- each public key can be used for signing for a maximum of two weeks
				- private keys are never directly accessible
			2. User managed service accounts
				- google only stores the public portion of a user managed key
				- users are responsible for private key security
				- can create upto 10 user-managed service account keys per service
				- can be administered via the IAM API, gcloud, or the console

		Note	
			- Each service has scopes
			- for user created service accounts, use IAM roles instead
			- google doesn't save user-managed private keys so they can't recover them
				- key rotation is managed by customer also

	Organization Restrictions
		- prevent data exfiltration through phishing or insider attacks
			- restricts access only to resources for authorized Google Cloud Organizations

		How it works?
			Managed Device(governed by company policies)
				-> Customer managed Egress proxy(Egress proxy admin  config proxy to add org restricitions headers to any req originating from managed device)
					-> Google Cloud Org/Vendor Google CLoud Org
						-> Cloud Storage


	IAM Best Practices
		1. Leverage and understand the resource hierarchy
			- use projects to group resources that share the same trust boundary
			- check the policy granted on each resource and make sure you understand the inheritance
			- use "principles of least privelege" when granting roles
			- audit policies in Cloud Audit Logs: setiampolicy
			- audit membership of groups used in policies
		2. Grant roles to Google groups instead of individuals
			- update group membership instead of changing IAM policy
			- audit membership of groups used in policies
			- control the ownership of the google group used in IAM policies
		3. Service Accounts Best Practices
			- Be very careful granting serviceAccountUser role
			- When you create a service account, give it a display name that cleary identifies its purpos
			- Establish a naming convention for service Accounts
			- Establish key rotation policies and methods
			- Audit w/ serviceAccount.keys.list() method
		4. Cloud Identity-Aware Proxy(IAP)
			- establish central authorization layer for app accessed by https
				- app level access control model instead of network level firewalls
					- means no VPN
			- enforce access control policies for app and resources:
				- identity-based access control
				- central authorization layer for app accessed by https
			- a security guard for your web app that controls access based on users identity/authenication and permission/authorization
			
		Note
				- IAM policy is applied after authentication.
	


	Qwiklabs - Exploring IAM
		https://googlecoursera.qwiklabs.com/focuses/34777833?parent=lti_session

		Objectives
			In this lab, you learn how to perform the following tasks:

				1. Use IAM to implement access control
				2. Restrict access to specific features or resources
				3 . Use the Service Account User role
	
		//list GCS bucket
			gcloud storage ls gs://[YOUR_BUCKET_NAME]

		Note
			- can grant group thus the members of the gfroup the role of Service Account User

	Quiz
	1. What abstraction is primarily used to administer user access in IAM ?
		-  Roles, an abstraction of job roles
			- IAN uses predefined roles for giving user access
			-	roles are define by more granukar permissios.
			- permissions are not applied to users directly, only through the roles that are assigned to them
	2. Which of the following is not a type of IAM role?
		- Advanced
		Type of IAM role:
			1. Basic
			2. Predefined
			3. Custom

	3. Which of the following is not a type of IAM member?
		- Organization Account 	
		Type of IAM member
			1. Google Account
			2. Cloud Identity domain
			3. Service Account
			4. Google Workspace domain
			5. Google group
			 
	Review
		- IAM builds on top of the Google Cloud services
		- Wokspace Admin/Cloud Identity
			- where creation of admin of corporate identities
			- handled by a person seprate from Google CLoud Administrator
		- Google Groups
			- establish a roles to all members
		- Wokspace admin
			- administers membership in the group
		- Service Accounts
			- enable you to build IaaS through terraform

Module 2 - Storage and Database Services
	- Implement data storage services in Google Cloud

	Learning Objectives
		1. Differentiate between Cloud Storage, Cloud SQL, Cloud Spanner, Firestore and Cloud Bigtable
		2. Choose a data storage service based on your requirements
		3. Implement data storage services

	Scope
		Infrastructure Track
			- Service differentiators
			- When to consider using each service
			- Set up and connect to a service
		
		Data Engineering Track
			- How to use a database system
			- Design, organization, structure, schema and use for an application
			- Details about how a service stores and retrieves structured data

	Cloud Storage
		- cloud's object storage.
			- a collection ob buckets that you place objects into
			- uses URL to access objects
		
		structure
			buckets(globally unique name)
				-> object
		Use case	
			- website content
			- storing data for archiing and DR
			- Distributing large data objects to users via direct download

		Key features
			- Scalable to exabytes

			- Time to first byte in milliseconds
			- Very high availbility across all storage classes
			- Single API across storage classes

		Classes
			1. Standard
				- frequently accessed data(hot)
				- stored for only brief period of time
				- no minimum storage duration and no retrieval constrain

				When to use
					- store data in same region as GKE clusters/GCE instance that uses it

			2. Nearline
				- infrequently accessed data like
					- data backup
					- long-tail multimedia content
					- data archiving
				- once a month

				When to use
					- lower availability than standard
					- 30 day min storage duration
					- costs for data access are acceptable trade-offs for lowered at-rest storage costs

			3. Coldline
				- cheaper infrequently accessed data than nearline
				- once in a 90 days
				When to use
					- lower availability than those 2 above
					- 90 day minimum storage duration
					- higher cost for data access are acceptable trade-offs for lowered at-rest storage costs

			4. Archive
				- once a year archiving storage data
				When to use
					- lowest cost
					- data archiving
					- online backup
					- DR
					- coldest service but data can be retrieved w/in milliseconds at higher expense
					- 365 day minimum storage duration
		
		Durability
			11 nines percent

		Location Types
			1. Multi-region
				- geo redundant
				- use for data serving around the world
					- website content
					- streaming videos
					- executing interactive workloads
					- serving data supporting mobile and gaming applicaionts
			2. Dual-Region
				- geo redundant
				- availability and DR

			3. Region
				- use w/ GKE/GCE instance that access it's data

		Modify after creation
			- can change storage data type
			- can't change location type
			- can change per object storage type/class
			- default class of bucket is applied to a new objects
			- objects can be moved from bucket to bucket
			- Object lifecycle Management
					- manage the classes of objects in your bucket
						- like s3 Intel tiering but on object level
					- with delete feature

		Security
			1. IAM
			2. Access control lists/ACL
				- for custom finer control
					- define who has access level to buckets/object
				- number entries is 100
					- scope	
						- define who(group/user) can perform specific action
							- collaborator@gmail.com
							- allUsers
								- anyone who is on the internet
							- allAuthenticatedUsers
								- represents anyone who is authenticated w/ google account
					- permission
						- define what actions(Read/write) can be performed
			3. Signed URLs
				- for more detailed control for granting internet user who don't use google account
					- means time-limited access to a bucket/object
				- "Valet key" access to buckets and objects via ticket:
					- Ticket is a cryptographically signed URL
					- Time-limited
					- Operations specified in ticket: HTTP/GET/PUT/DELETE and not POST
					- Any user with URL can invoke permitted operations

				Signed Policy documents
					- for determining what kind of file can be uploaded by someone w/ signed URL
				- URL is signed using private key associated w/ service account
					- service account delegates its trust of the account to the holder of the URL
				
				Note
					- signed URL expiration should be a reasonable time

	Cloud Storage features
		1. Customer-Supplied Encryption Key(CSEK)
			- use your own key instead of Google managed keys
		2. Object Lifeccyle management
			- automatically delete/archive objects
		3. Object versioning
			- maintain multiple versions of objects
				- charged each versions as if they were multiple files
			- supports retrieval of objects that are deleted or overwritten
			- maintain a history of modifications of objects
				- older version is stored in archive with added generation number to its name
				- turning it off will retain archive stored
			- list archived versions of an object, restore an object to an older state/delete a version
			
			Support configuration
				- object inspection occurs in asynchronous batches
				- changes can take 24 hrs to apply
				
				When to use
					- downgrade storage class on objects older than a year/archiving older versions of objects/downgrading
						- downgrade to coldline for older than ayear
					- delete objects created before a specific date
					- keep only the 3 most recent versions of an object
			
		4. Directory synchronization
			- synchronizes a VM directory w/ a bucket
		5. Object change notifications using Pub/Sub
		6. Autoclass
			- manages all aspects of storage class for  a bucket

		Notes
			- objects are immutable means uploaded object cannot be modified
		
		database import services
		  - use for transfering petabytes/terabytes
			- Transfer Appliance/Snowcone-Snowmobile
				- rack, capture and then ship your data to google cloud
			- Storage transfer service
				- import online data through online
			- Offline Media Import
				- Third party provider uploads the data from physical media

		Global consistency
			- read after write
			- read after metadata-update
			- read after delete
			- bucket listing
			- object listing

	Choosing a storage class
		Structured Data
			- choose DB services
		
		Unstructured Data
			- choose GCS
				- Read < once a year
					- archive
				- Read once per 90 days
					- coldline
				- Read once per 30 days
					- nearline
				- Read less than 30 days
					- standard
				
				Location type
					- Region
						- same as customer for:
							- less latency and optimize network BW for data consumers
								- analytics pipelines grouped in same region
					- Dual region
						- want same performance as region with higher availability(geo-redundant)
					- Multi-region
						- serve content to data consumers outside of google network and distributed across large geographic areas
						- higher data availibility that comes with being geo-redundant

				- Autoclass
					- bucket level
					- like Amazon s3 Intelligent- Tiering
						- auto cost savings by moving objects between tiers based on access patterns.		
					- variety of access frequencies/unknown/predictible
					- No charged	
						- storage class transitions
						- no retrieval charges
						- no early deleteion charges
	
	Qwiklabs: Cloud Storage
		https://googlecoursera.qwiklabs.com/focuses/34814610?parent=lti_session
	
	Objectives
		In this lab, you learn how to perform the following tasks:
			- Create and use buckets
			- Set access control lists to restrict access
			- Use your own encryption keys
			- Implement version controls
			- Use directory synchronization
			- Share a bucket across projects using IAM

		Access control lists
			// store bucket name into a variable
				export BUCKET_NAME_1=bineliasray
			
			// verify
				echo $BUCKET_NAME_1
			
			// copy first file to the bucket
				gcloud storage cp setup.html gs://$BUCKET_NAME_1/
			
			// Get default access list
				gsutil acl get gs://$BUCKET_NAME_1/setup.html  > acl.txt
cat acl.txt
	
			// set access list to private
				gsutil acl set private gs://$BUCKET_NAME_1/setup.html
			
			// verify results
				gsutil acl get gs://$BUCKET_NAME_1/setup.html  > acl2.txt
cat acl2.txt

			// update ACL to make publicly readable
				gsutil acl ch -u AllUsers:R gs://$BUCKET_NAME_1/setup.html

			// verify
				gsutil acl get gs://$BUCKET_NAME_1/setup.html  > acl3.txt
cat acl3.txt

		Customer-supplied encryption keys(CSEK)
			// Create a CSEK: AES-256 base-64 key
				python3 -c 'import base64; import os; 
				print(base64.encodebytes(os.urandom(32)))'

			//generate boto file
				gsutil config -n

			// check gsutil version
				gsutil version -l 

			// copy the key to boto fole encryption key variable
			 encryption_key=2dAtVPvt9dCVKPkilGPcRGtlrXQ1C0gFgJrl8/XVWDY=

		Rotate CSEK keys
			// use above key for decryption_key1
				decryption_key1=2dAtVPvt9dCVKPkilGPcRGtlrXQ1C0gFgJrl8/XVWDY=

			// generate new key for encryption_key and update .boto file
				encryption_key=ZougR707S+PRRMOjLGDOcNk2spvJ+H6mumVqom7yqnU=

			// rewrite the key for file 1/setup2 and comment out the old decrypt key
				gsutil rewrite -k gs://$BUCKET_NAME_1/setup2.html

		Enable lifecycle management
			// view current lifecycle policy 
				gsutil lifecycle get gs://$BUCKET_NAME_1

			// Create JSOn lifecycle policy file
				vi life.json
					{
						"rule":
						[
							{
								"action": {"type": "Delete"},
								"condition": {"age": 31}
							}
						]
					}
			
			// set policy
				gsutil lifecycle set life.json gs://$BUCKET_NAME_1

			// verify
				gsutil lifecycle get gs://$BUCKET_NAME_1

			// check of enable versioning is enabled
				gsutil versioning get gs://$BUCKET_NAME_1

			// enable versioning
				gsutil versioning set on gs://$BUCKET_NAME_1

		Create serveral versions of sample file in bucket
			//delete five lines
			from 59 kb to 57.1kb
			
			//upload to GCS bucket
				gcloud storage cp -v setup.html gs://$BUCKET_NAME_1
			
			//list all versions of the file
				gcloud storage ls -a gs://$BUCKET_NAME_1/setup.html

			//oldest
			gs://bineliasray1/setup.html#1710722349381314
			
			//newest
			gs://bineliasray1/setup.html#1710723107639739

		Synchronize a directory and sync w/ a bucket
			//Make a nested directory structure so that you can examine what happens when it is recursively copied to a bucket.
				mkdir firstlevel
				mkdir ./firstlevel/secondlevel
				cp setup.html firstlevel
				cp setup.html firstlevel/secondlevel

			//To sync the firstlevel directory on the VM with your bucket, run the following command:
				gsutil rsync -r ./firstlevel gs://$BUCKET_NAME_1/firstlevel

			//Verify
				gcloud storage ls -r gs://$BUCKET_NAME_1/firstlevel

		Cross-project sharing
			//Create service account in project 2
				cross-project-storage
				//permission
					cloud storage>Storage object viewer
			
			//Verify by connecting to it from a VM in project 1
				//upload key creadential.json
				//authorize VM
					gcloud auth activate-service-account --key-file credentials.json

				//verify access
					//list object in buckets
					gcloud storage ls gs://$BUCKET_NAME_2/

					//upload file to bucket 2
					gcloud storage cp credentials.json gs://$BUCKET_NAME_2/
	
	Filestore
		- managed file storage service that requires file sys interface and shared file syst
		- fuly managed network attached storage(NAS) for Compute Engine and GKE instances
		- predictable performance
		- full NFSv3 support
		- scale to 100s of TBs for high performance workloads

		Usecase
			- application migrations
			- media rendering
				- enabling visual effects artists to collaborate on same fileshare as rendering workflows typically run across fleets of compute machines/shared file sys
			- electronic design automation(EDA)
				- data management
				- unirversally accessiblw
			- data analytics
				- compute complex financial models or analysis of environmental database
			- Genome sequencing
			- Research institutions performing scientific research
			
	Cloud SQL
		- managed structured/relational DB services.
		- MySQL/PostgreSQL/Microsoft SQL Server DB
		- auto patches and updates
			- but you still have to administer MySQL users w/ native authentication tools
		- supports
			- cloud shell
			- app engine
			- google workspace scripts
			- SQL workbench
			- toad
			- standard MySQL drivers
		- performance
			- upto 64TB storage capacity
			- 60,000 IOPS
			- 624 GB of RAM per instance
			- scale up to 96 vCPU
			- scale out w/ read replicas

		Cloud SQL services
			- High availability setup
				- one primary instance
					- write only
					- persistent disk 1
						- sync replication  from regional persistent disk
				- one standby instance
					- only read only
					- persistend disk 2
						- sync replication  from regional persistent disk
				- one regional persistent disks
					- attach to both primary instance and standby instance
			- Backup service
			- Import/Export
			- Scaling
				- up: Machine capacity
					- require machine restart
				- out: Read replicas
					- horizontal scalibility
						- consider cloud spanner

			Auto failover
				- primary instance is down, standby instance will be the new primary and users rerouted to that instance
		
		Recommended
			- Security 
				- app in the same region sa cloud SQL
					- cloud SQL can use private IP connection
				- if app is in different region/project
					1. Cloud SQL Auth Proxy
						- automated SSL cert that handles:
							- authentication
							- encryption
							- key rotation
					2. Manual SSL connection 
						- need manual control over SSL cert
							- you rotate cert yourself
					3. Authorized networks
						- cannot use SSL
						- authorizes specific IP addr to connect http external IP addr

		Qwiklabs - Implementing Cloud SQL
			https://googlecoursera.qwiklabs.com/focuses/34832957?parent=lti_session

			Objectives
			In this lab, you learn how to perform the following tasks:

				1. Create a Cloud SQL database
				2. Configure a virtual machine to run a proxy
				3. Create a connection between an application and Cloud SQL
				4. Connect an application to Cloud SQL using Private IP address

				username
					student-04-b93d4607a686@qwiklabs.net
				password
					CqNhmzDiQLg5

				//A few points to consider for machine type config:
					1. Shared-core machines are good for prototyping, and are not covered by Cloud SLA.
					2. Each vCPU is subject to a 250 MB/s network throughput cap for peak performance. Each additional core increases the network cap, up to a theoretical maximum of 2000 MB/s.
					3. For performance-sensitive workloads such as online transaction processing (OLTP), a general guideline is to ensure that your instance has enough memory to contain the entire working set and accommodate the number of active connections.

				//A few points to consider for storage config:
					1. SSD (solid-state drive) is the best choice for most use cases. HDD (hard-disk drive) offers lower performance, but storage costs are significantly reduced, so HDD may be preferable for storing data that is infrequently accessed and does not require very low latency.
					2. There is a direct relationship between the storage capacity and its throughput.

			Configure a proxy on a virtual machine
				Two VM
					one Wordpress VM private IP
					one worpress proxy VM
						//Download the Cloud SQL Proxy and make it executable
							wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 -O cloud_sql_proxy && chmod +x cloud_sql_proxy

				Cloud SQL instance
					//Instance connection name
						qwiklabs-gcp-00-8103347d7842:us-east1:wordpress-db

					//To activate the proxy connection to your Cloud SQL database and send the process to the background, run the following command:
					./cloud_sql_proxy -instances=$SQL_CONNECTION=tcp:3306 &

				Connect an application to the Cloud SQL instance
					//Configure the Wordpress application. To find the external IP address of your virtual machine, query its metadata:
						curl -H "Metadata-Flavor: Google" http://169.254.169.254/computeMetadata/v1/instance/network-interfaces/0/access-configs/0/external-ip && echo

				Connect to Cloud SQL via internal IP
					Note
						- host your application in the same region and VPC connected network as your Cloud SQL, you can leverage a more secure and performant configuration using Private IP.
						- Notice that this time you are creating a direct connection to a Private IP, instead of configuring a proxy. That connection is private, which means that it doesn't egress to the internet and therefore benefits from better performance and security.

					//private IP
						10.58.0.2

				Overview
					you created a Cloud SQL database and configured it to use both an external connection over a secure proxy and a Private IP address, which is more secure and performant. Remember that you can only connect via Private IP if the application and the Cloud SQL server are collocated in the same region and are part of the same VPC network. If your application is hosted in another region, VPC, or even project, use a proxy to secure its connection over the external connection.


	Cloud Spanner
		- managed relational data workload not analytics but having 99.999% availability
		- scale to petabytes
			- auto failover
			- horizontal Scaling
			- auto replication/synchrnous replication
		- high availability
			- multi-regional: five 9's
			- regional: four 9's
		
		Usecase
			- used for financial and inventory applications 
					- transactions and inventory management

	Cloud Spanner Architecture
		- replicates data in end cloud zones within one region/across several regions
			- high availability and global placement
			- uses global fiber network for synch across region
		- uses atomic clocks ensures atomiscity when updating data

	When to use
		- Outgrown single instance RDBMS
		- sharding for DB throughput
		- need transactional consistency
		- global data + strong consistency
		- DB consolidation

	Note:
		- need full relational capability
			- use Cloud SQL
			
	AlloyDB
		- fully postgreSQL DB service
			- auto:
				- backups
				- replication 
				- patching
				- capacity management

		When to use
			 - hybrid transactional and analytical processing
			 - high/fast transactional processing 
			 	- more than four times faster than standard PostgreSQL
			 - high availability
			 		- 99.99% uptime SLA
			- multiple read replicas
			- inclusive maintenance
			- real-time business insights for analytical queries
				- built in vertex AI
	
	Cloud Firestore
		- fully managed serverless nosql document DB 
			- simplifies storing, syncing and querying data
			- mobile, web and IOT apps at global scale
			- live synchronization and offline support
			- security features
			- supports ACID transactions
				- if any of the operations fail and cannot retried, the whole transaction will fail
			- multi-region replication
			- powerful query engine

		- next generation of Cloud Datastore

		Datastore mode
			- compatible w/ datastore applications
			- strong consistency
			- no entity group limits
			
			When to use
				 - new server projects

		Native mode
			- strongly consistent storage layer
			- collection and document data model
			- real-time updates
			- mobile and web client libraries
		
		When two use
				- new:
					- mobile apps
					- web apps

		Decision tree
			Consider Cloud Firestore	
				- schema might change
				- need adaptable DB
				- scale down to zero
				- low maintenance overhead
				- scale up to TB 
			
			Consider Cloud Bigtable
				- don't require transactional consistency

	Cloud Bigtable
		- fully managed noSQL DB w/ PB scale and very low latency
		- seamless scaling for throughput
		- learn to adjust to specific access patterns
			- throughput scales linearly
		- supports high read/write throughput at low latency
			- great storage engine for machine learning app
			- easy integration w/ open source big data tools
				- Hadoop
				- cloud dataflow
				- cloud dataproc
				- HBase API
		
		Usecases
			- operation and analytical applications
			- IoT
			- user analytics
			- financial data analysis

		Cloud Bigtable storage model
			column family
				column qualifier
					- used as data
			row key

		Processing is separated from storage
			- clients
			- processing
				- bigtable nodes
				- tablets
					- sharded into blocks of contiguous rows
			- storage
				- colossus file system
					- google file sys
					- SSTable format
						- persistent, ordered immutable map from keys to values
							- arbitrary byte strings
		
		Decision tree
			Consider Cloud Bigtable
				- bigtable scales up well
				- storing > 1TB of unstrucutred data
				- very volumes of high volume of writes
				- need read/write latency of less than 10ms and strong consistency
				- HBase API compatibility

			Consider Cloud Firestore
				- firestore scales down well

			Note:
				- Smallest cloud bigtable cluster creation
					- three nodes that can handle 30,000 operations per second
						- pay per operational node
							- means you pay whether you're using them or not

	Memorystore
		- fully managed in-memory data store service for:
			- workloads requiring:
				- microsecond response times
				- large spikes in traffics
					- gaming environments
					- real-time analytics

	BigQuery
		- relational data workload used primary for analytics

	Cloud SQL
		- relational data workload not analytics and not 99.999% availability

	Quizes
	1. What data storage service might you select if you just needed to migrate a standard relational database running on a single machine in a datacenter to the cloud?
		- Cloud SQL
	2. Which Google Cloud data storage service offers ACID transactions and can scale globally?
		- Cloud Spanner
	3. Which data storage service provides data warehouse services for storing data but also offers an interactive SQL interface for querying the data?
		- BigQuery
			- data warehousing service that allows storage of huge data sets while making them immediately processable w/o having to extract or run the processing in a separate service

	Review
		1. Cloud Storage
			- fully managed object storage
		2. Filestore
			- fully managed file storage
		3. Cloud SQL
			- fully managed MySQL and PostgreSQL DB
		4. Cloud Spanner
			- relational DB service w/ transactional conistency, global cale and high availbility
		6. AlloyDB
			- fully managed postgreSQL compatible DB service
		7. Firestore
			- fully managed NoSQL document DB
		8. Cloud Bigtable	
			- fully managed noSQL wide column DB
		9. Memorystore
			- fully managed in-momemory data store servicefor Redis
		

Module 3 - Resource Management
	- Manage and examine billing of Google Cloud resources

	Learning Objectives
		1. Describe the cloud resource manager hierarchy
		2. Recognize how quotas protect Google Cloud customers
		3. Organize resources using labels
		4. Explain the behavior of budget alerts in Google Cloud
		5. Examine billing data with BigQuery

	Resource Manager
		- hierarchically manage resources by project, folder, and organization

		Note
			- IAM policies are inherited top-to-bottom
				- IAM allow and deny policies
			- Billing and Resource Monitoring are accomulated from the bottom up
				- Org contains all billing accounts
				- project is associated w/ one billing account
				- a resource belongs to one and only one project

		Project creator
			why
				project accumulates consumption of all its resources
					- can be used to track resources and quota usage

			track resource and quota usage
				- enable billing
				- manage permission and credentials
				- enable service and APIs
				- billing and reporting is per project

			Project use 3 identifying attributes
				- proj name
				- proj number
				- project ID/app ID

			Resource hierarchy
				- Global/project
					- images
					- snapshots
					- networks
				- regional/project
					- external IP addresses
				- zonal/project
					- instances 
					- disks

	Quotas
		All respirces are subject to project quotas or limits/3 categories
			1. How many resource you can create per project?
				- 15 VPC networks/project
			2. How quickly you can make API requests in a project: rate limits?
				- 5 admin actions/sec(Cloud Spanner rate limits)
			3. How many resources you can create per region?
				- 24 CPus region/prokect
			
		How to increase?
			1. As you use google cloud expands over time, your quotas may increase accordingly
			2. Request quota adjustments
				- quotas page in the google cloud console or a support ticket

		Why use project quotas?
			- project quotas prevent runaway consumption in case of error or malicious attacks
			- prevent billing spikes/suprises
			- forces sizing consideration and periodic review 
	
	Labels
		- utility for organizing google cloud resources by label filtering
		- key value pairs that can be attach to resources:
			- VMs
			- disks
			- snapshots

		How?
			- google cloud console
			- gcloud
			- Resource Manager API

		limits
			- 64 labels/resources

		Example
			- inventory
			- file resources
			- scripts
				- help analyze costs
				- run bulk operators
			- department/team/Cost center
				- dept:marketing
				- dept:engineering
			- components
				- component:Redis
				- componet:fe
			- owner/contact
				- owner:gaurav
				- contact:opm
			- environment/storage
				- environment:prod
				- environment:test
			- state
				-state:active
				-state:readyfordeletion

		Comparing labels and transfering
			Labels
				- are a way to organize resources across Google Cloud
					- disks/image/snapshots...
					- user-defined strings in key-value format
					- propagated through billing
			Tags
				- applied to instances only
					- user-defined strings
					- primarily used for networking(Firewall rules)
	
	Billing
		- one billing account per projects

		Controlling costs features	
			Setting budget
				- Scope
					- name
					- projects
				- amount
					- budget type
					- target amt
				- Actions
					- percent of budget
					- amt
					- trigger on
				- alerts
					- email
					-  Pub/Sub -> Cloud functions
			
			Looker Studo/Data studio
				- visualing google cloud spend in billing dashboard
				 - easy to read/share
				 - fully customizable

			BigQuery	
				- analyze your spend
				- fully managed enterprise data warehouse w/ SWL and fast response times
			
		Note
			- labels can help you optimize google cloud spend

	Qwiklabs: Examining Billing data with BigQuery
		- In this lab, you learn how to use BigQuery to analyze billing data.
		- https://googlecoursera.qwiklabs.com/focuses/34833617?parent=lti_session

		Objectives
			In this lab, you learn how to perform the following tasks:
				1. Sign in to BigQuery from the Cloud Console
				2. Create a dataset
				3. Create a table
				4. Import data from a billing CSV file stored in a bucket
				5. Run complex queries on a larger dataset

		Locate the row that has the Description: Network Internet Ingress from EMEA to Americas.
		What was the total consumption and units consumed?
			- 9,738,199 bytes
			
		Locate the row that has the Description: Network Internet Egress from Americas to China.
		Can you interpret the information?
			- 5,542 bytes exited the Americas and was transferred to China at a charge of 1e-06.

		Compose a simple query
			SELECT * FROM `imported_billing_data.sampleinfotable`
			WHERE Cost > 0

			How many rows had cost greater than 0?
			 - 20 rows

		Analyze a large billing dataset with SQL
			SELECT
  			product,
				resource_type,
				start_time,
				end_time,
				cost,
				project_id,
				project_name,
				project_labels_key,
				currency,
				currency_conversion_rate,
				usage_amount,
				usage_unit
			FROM
				`cloud-training-prod-bucket.arch_infra.billing_data`
		
		find the latest 100 records where there were charges (cost > 0), for New Query,
			SELECT
				product,
				resource_type,
				start_time,
				end_time,
				cost,
				project_id,
				project_name,
				project_labels_key,
				currency,
				currency_conversion_rate,
				usage_amount,
				usage_unit
			FROM
				`cloud-training-prod-bucket.arch_infra.billing_data`
			WHERE
				Cost > 0
			ORDER BY end_time DESC
			LIMIT
				100

		find all charges that were more than 3 dollars, for Compose New Query, paste the following in Query Editor:
			SELECT
				product,
				resource_type,
				start_time,
				end_time,
				cost,
				project_id,
				project_name,
				project_labels_key,
				currency,
				currency_conversion_rate,
				usage_amount,
				usage_unit
			FROM
				`cloud-training-prod-bucket.arch_infra.billing_data`
			WHERE
				cost > 3

		To find the product with the most records in the billing data, for New Query, paste the following in Query Editor:
			SELECT
				product,
				COUNT(*) AS billing_records
			FROM
				`cloud-training-prod-bucket.arch_infra.billing_data`
			GROUP BY
				product
			ORDER BY billing_records DESC

		Which product had the most billing records?
			- Cloud Pub/Sub has 10,271 records

		To find the most frequently used product costing more than 1 dollar, for New Query, paste the following in Query Editor:
			SELECT
				product,
				COUNT(*) AS billing_records
			FROM
				`cloud-training-prod-bucket.arch_infra.billing_data`
			WHERE
				cost > 1
			GROUP BY
				product
			ORDER BY
				billing_records DESC

		Which product had the most billing records of over $1
			- Compute Engine has 17 charges costing more than 1 dollar.

		To find the most commonly charged unit of measure, for Compose New Query, paste the following in Query Editor:
Compute Engine has 17 charges costing more than 1 dollar.
			
		What was the most commonly charged unit of measure?
		- Byte-seconds were the most commonly charged unit of measure with 2,937 requests.
		
		To find the product with the highest aggregate cost, for New Query, paste the following in Query Editor:
			SELECT
				product,
				ROUND(SUM(cost),2) AS total_cost
			FROM
				`cloud-training-prod-bucket.arch_infra.billing_data`
			GROUP BY
				product
			ORDER BY
				total_cost DESC

			Which product has the highest total cost?
			- Compute Engine has an aggregate cost of $112.02.

		Note
			- If the project ID is not specified, BigQuery will default to the current project.

	Quiz
	1. No resources in Google Cloud can be used without being associated with...
		- A project
	2. A budget is set at $500 and an alert is set at 100%. What happens when the full amount is used?
		- A notification email is sent to the Billing Administrator.
	3. How do quotas protect Google Cloud customers?
		- By preventing uncontrolled consumption of resources.

	Review
		- analyze spending data with BigQuery
		- reporting
			- establish accountability
		- Transparency
			- principle og google cloud

Module 4 - Resource Monitoring
	- Monitor resources using Google Cloud's operations suite

	Learning Objectives
		1. Describe Google Cloud's operations suite for monitoring, logging, error reporting, tracing, and debugging
		2. Create charts, alerts, and uptime checks for resources with Cloud Monitoring
		3. Identify and fix errors using Cloud Debugger

	Google Cloud's Operations suite/Stackdriver
		- a service agent that provides monitoring, logging, error reporting, trace, profiler and diagnostics for your applications
		- manages across platforms
			- Google cloud and AWS
			- dynamic discovery of google cloud with smart defaults
			- open source agents and integrations
		- access to powerful data and analytics tools
		- collaboration w/ third-party software

	Monitoring
		- base of SRE.
		- dynamically  configures monitoring after resources are deployed
		- has intelligent defaults that allow you to easily create charts for basic monitoring activities
		- monitors platform, system and application metrics
			- ingesting data
				- metrics, events, metadata
			- generates insights through dashboards, charts, alerts
		
		Integrated services	
			- app engine
			- compute engine instance
			- url of host
			- AWS instance/load balancer

		Example
			-  Uptime/health checks
				- have Dashboard
				- sends alerts via email
		
		site reliability engineering(SRE)
			- applies aspect of software engineering to operations whos goals are
				- create ultra-scalble and reliable software systems
			- enable google to build, deploy, monitor and maintain some e of the largest software sys in the world

			hierarchy
				1. product
				2. development
				3. capacity planning
				4. testing+releasse procedures
				5. postmortem/RCA
				6/. Incident response
				7. Monitoring
	
		Note:
			- metric scope 
				- is the root entity that holds monitoring and configuration information
					- scoping project
						- monitoring
						- dashboards
						- uptime checks
							- configured to test availability of your public services from locations around the world
							- create alerting policy and view the latency of each global location
							Sample
								- HTTP uptime check
									- checked every minute w/ 10-s timeout
						- configurations
				- is a "single pane of glass"
			-  metrics data and log entries remain in the individual project
			- charts of CPu utilization/oacjkets ents and received and packets dropped by firewall
				- provides visibility into the utilization and network traffic of your VM instaces

		Best Practices when creating alerts
			1. Alerting on symptoms and not necessarly the causes
				- monitor failing queries of DB and then identify whether the DB is down
			2. Use multiple notif channel like email and SMS to avoid single point of failure
			3. Customize alerts to audience's needs 
				- describe what actions need to be taken
				- what resources need to be examined
			4. Avoid noise 
			5. Adjust monitor alerts so that they are actionalble

		Monitoring VM 
			Ops agent
				- gathers sys and app metrics from VM instances and sends them to Monitoring
				- can monitor 3rd party app
					- apache
					- mysSQL
					- oracle DB
					- SAP HANA
					- nginx


	Qwiklabs: Resource monitoring
		https://googlecoursera.qwiklabs.com/focuses/34836054?parent=lti_session
		- you learn how to use Cloud Monitoring to gain insight into applications that run on Google Cloud.

		Objectives
		In this lab, you learn how to perform the following tasks:

			1. Explore Cloud Monitoring
			2. Add charts to dashboards
			3. Create alerts with multiple conditions
			4. Create resource groups
			5. Create uptime checks

		Why is monitoring important to Google?
		 - It is at the base of site reliability which incorporates aspects of software engineering and applies that to operations whose goals are to create ultra-scalable and highly reliable software systems.


		What is not a recommended best practice for alerts?
			- Report all noise to ensure all data points are presented.

			Recommendation
				- Use multiple notification channels so you avoid a single point of failure.
				- Configure alerting on symptoms and not necessarily causes.
				- Customize your alerts to the audience need
		
		Select all valid targets for Cloud Monitoring uptime alert notifications.
			- pub/sub
			- email
			- webhook
			- sms
			- 3rd party service

	Logging
		- allows you to store, search, analyze, monitor and alert on log data and events from google cloud and AWS
		- a fully managed service that performs at scale and can ingest platform/systems/app log data from thousands of services
			- API to write to Logs
			- 30 day retention
		- can do log search/view/filtering
		- can create log-based metrics
		- can export to:
			- GCS
			- BigQuery
				- create easy-ti-understand reports and dashboards
			- Pub/Sub
				- enables you to stream logs to apps or endpoints

			Why?
				-analyze logs and visualize in Looker studio	
					- network traffics
						- forecast capacity growth
					- network usage
						- optimize network expensis
					- network forensics
						- analyze incidents



	Error Reporting
		- counts, analyzes and aggregates and display errors in your running cloud services
			- Error notification
			- Error Dashboard 
				- sorting
				- fitering

		Integrated Services
			- app engine
			- app scripts
			- compute engine
			- cloud functions
			- cloud run
			- GKE
			- Amazon EC2

		Supports
			- Go
			- java
			- .Net
			- Node.js
			- PHP
			- python
			- ruby

	Tracing
		- provides latency sampling and reporting for Google App Engine, Google HTTP(S) load balancers, and applications instrumented with the Cloud Trace SDKs. 
		Reporting includes per-URL statistics and latency distributions
		- collects per URL latency data and displays it in near real-time.
		- automaticall analyzes all data and generate in depth latency reports for suface performance
		- used to keep services running at extreme scale

		Why?
			Managing the amt of time of latency is important for application performance.

		Integrated Service
			- App Engine
			- Google HTTP(S) load balancers
			- Applications instrumented w/ the Cloud Trace SDKs
	
	Profiling
		- continuously analyze the performancd of CPU/RAM intensive functions exec across app
		- uses statistical techniques and extremely low-impact instances
		- runs across all production instance to provide complete picture of app performance w/o slowing instances
		
		Supports:
			- java
			- goals
			- node.js
			- python

		Why?
			- need to fix poorly performing code that increases latency and cost of app of web services everyday

	Qwiklabs:Error Reporting and Debugging
		https://googlecoursera.qwiklabs.com/focuses/34836054?parent=lti_session
		- In this lab, you learn how to use Cloud Error Reporting and integrate Cloud Debugger.

		Objectives
		In this lab, you learn how to perform the following tasks:

			1. Launch a simple Google App Engine application
			2. Introduce an error into the application
			3. Explore Cloud Error Reporting
			4. Use Cloud Debugger to identify the error in the code
			5. Fix the bug and monitor in Cloud Operations
		
		To create a local folder and get the App Engine Hello world application, run the following commands:
			mkdir appengine-hello
			cd appengine-hello
			gsutil cp gs://cloud-training/archinfra/gae-hello/* .

		To run the application using the local development server in Cloud Shell, run the following command:
			dev_appserver.py $(pwd)

		// Create an app in app engine
			https://cloud.google.com/appengine/docs/flexible/python/create-app

		Deploy the application to App Engine
			//To deploy the application to App Engine, run the following command:
				gcloud app deploy app.yaml

			//verify that the application is working by running the following command:
				gcloud app browse

			

		Introduce an error to break the application
			//To re-deploy the application to App Engine, run the following command:
				gcloud app deploy app.yaml --quiet

					The --quiet flag disables all interactive prompts when running gcloud commands. 
					If input is required, defaults will be used. In this case, it avoids the need for you to type Y when prompted to continue the deployment.

			//verify
				gcloud app browse

			Which service requires a logging agent installed to collect and send logs to Cloud Operations?
				- Compute Engine

		Explore Cloud Error Reporting
			Which service(s) are currently supported by Cloud Error Reporting?
				- App Engine Standard
				- Kubernetes
				- App Engine Flexible
				- Compute Engine

			What would not be considered a benefit of Cloud Operations?
				- Boosts all network performance

			Benefit of error reporting
				- Faster problem resolution
				- Multi-cloud monitoring
				- Reduces monitoring overhead

	Quiz
	1. What is the foundational process at the base of Google's Site Reliability Engineering (SRE) ?
		- Monitoring.
			- Before you can take any of the other actions, you must first be monitoring the system.
	2. What is the purpose of the Cloud Trace service?
		- Reporting on latency as part of managing performance.
	3. Google Cloudâ€™s operations suite integrates several technologies, including monitoring, logging, error reporting, and debugging that are commonly implemented in other environments as separate solutions using separate products. What are key benefits of integration of these services?
		- Reduces overhead, reduces noise, streamlines use, and fixes problems faster
			- Integration with Google Cloudâ€™s operations suite streamlines and unifies these traditionally independent services, making it much easier to establish procedures around them and to use them in continuous ways.
	
	Review
		Site Reliability Engineering
			- operate and maintain you app through monitoring, loggin, error reporting, fault tracing and profiling features
			
Essential Google Cloud Infrastructure: Scaling and Automation

Module 1 - Interconnecting Networks
  - connect your Infrastructure to  Google Cloud

  Learning Objectives
    1. Recall the Google Cloud interconnect and peering services available to connect your infrastructure to Google Cloud
    2. Determine which Cloud Interconnect or peering service to use in specific circumstances
    3. Create and configure Google Cloud HA VPN
    4. Recall when to use Shared VPC and when to use VPC Network Peering

  Cloud VPN
    Cloud VPN
      - securely connects your on-premises network to yhour google cloud VPC network

    Two Cloud VPN gateways  
      1. HA VPN
        - high availability cloud VPN sol'n that lets you securely connect on-premise network 
          to your VPC network through an IPsec VPN connection in a single region.
        - SLA if 99.99% service availability

        Supports site-to-site VPN for different topologies/configuration scenarios:
          - An HA VPN gateway to peer VPN devices
          - An HA VPN gateway to an AWS virtual private gateway
          - Two HA VPN gateways connected to each other

        How for Google Cloud to On-premise?
          - Cloud HA VPN Gateway
            - google cloud auto chooses two external IP
               - supports multiple tunnels
          - Cloud Router
          - Two on premise VPN gateway
          - Two or four VPN tunnels
            - serves as the virtual medium through IPsec encrypted data
            - must be paired
            - VPN tunnels connected to HA VPN gateways must use dynamic(BGP) routing
              - active/active routing configurations
              - active/passive routing configurations

        How for Google Cloud to AWS Cloud?
          - two tunnels from AWS TGW/VPG to one interface of HA cloud VPN gateway
          - other two tunnels from AWS TGW/VPG to second interface of same HA cloud VPN gateway

        HA VPN between Google Cloud networks topology
          - each

        Dynamic routing with Cloud router
          - auto/managed routes w/o chaning tunnel config usign BGP

          How?
            - BGP link local IP addr bet cloud router and peer gaweway
            - two VPN tunnel
            - one Google VPN gateway

      2. Classic VPN
        - securely connects your onpremises network to your google cloud vpc network through IPsec VPN tunnel
        - data at transit is encrypted by one VPN gateway and decrypted by other gateway
        - SLA of 99.9% sevice availability
        
        Supports:
          - site-to-site VPN
          - static routes
          - dynamic routes(cloud Router)
          - IKEv1 and IKEv2 ciphers
        
        Doesn't support
          - dial in to a VPN using client VPN software

        When to use?
          - low volume data connection

        How?
          - Cloud VPN Gateway
            - google cloud regional external IP
          - On-premises VPN Gateway
            - has an external IP
            - Maximum Transimission Unit cannot be greater than 1460 bytes
          - Two VPN tunnels
            - serves as the virtual medium through IPsec encrypted data
            - must be paired

    HA VPN

    Qwiklab: Configuring Google Cloud HA VPN
      https://googlecoursera.qwiklabs.com/focuses/34859444?parent=lti_session
      - HA VPN is a high-availability (HA) Cloud VPN solution that lets you securely connect your on-premises network to your VPC network through an IPsec VPN connection in a single region. HA VPN provides an SLA of 99.99% service availability.
      - HA VPN is a regional per VPC, VPN solution. HA VPN gateways have two interfaces, each with its own public IP address. When you create an HA VPN gateway, two public IP addresses are automatically chosen from different address pools. When HA VPN is configured with two tunnels, Cloud VPN offers a 99.99% service availability uptime.
      - In this lab you create a global VPC called vpc-demo, with two custom subnets in us-east4 and us-central1. 
        In this VPC, you add a Compute Engine instance in each region. 
        You then create a second VPC called on-prem to simulate a customer's on-premises data center. 
        In this second VPC, you add a subnet in region us-central1 and a Compute Engine instance running in this region. 
        Finally, you add an HA VPN and a cloud router in each VPC and run two tunnels from each HA VPN gateway before testing the configuration to verify the 99.99% SLA.

      Objectives
      In this lab, you learn how to perform the following tasks:

        1. Create two VPC networks and instances.
        2. Configure HA VPN gateways.
        3. Configure dynamic routing with VPN tunnels.
        4. Configure global dynamic routing mode.
        5. Verify and test HA VPN gateway configuration.

      //list active acct
        gcloud auth list

      //list active proj
        gcloud config list project

      Task 1. Set up a Global VPC environment
        //create a VPC network called vpc-demo
          gcloud compute networks create vpc-demo --subnet-mode custom

          //create subnet vpc-demo-subnet1 in the region us-central1
            gcloud compute networks subnets create vpc-demo-subnet1 \
--network vpc-demo --range 10.1.1.0/24 --region "us-central1"
  Cloud Interconnect and Peering

          //Create subnet vpc-demo-subnet2 in the region us-east4
            gcloud compute networks subnets create vpc-demo-subnet2 \
--network vpc-demo --range 10.2.1.0/24 --region us-east4

          //Create a firewall rule to allow all custom traffic within the network:
            gcloud compute firewall-rules create vpc-demo-allow-custom \
  --network vpc-demo \
  --allow tcp:0-65535,udp:0-65535,icmp \
  --source-ranges 10.0.0.0/8

          //Create a firewall rule to allow SSH, ICMP traffic from anywhere:
            gcloud compute firewall-rules create vpc-demo-allow-ssh-icmp \
    --network vpc-demo \
    --allow tcp:22,icmp

          //Create a VM instance vpc-demo-instance1 in zone us-central1-b:
            gcloud compute instances create vpc-demo-instance1 --machine-type=e2-medium --zone us-central1-b --subnet vpc-demo-subnet1

          //Create a VM instance vpc-demo-instance2 in zone us-east4-b:
            gcloud compute instances create vpc-demo-instance2 --machine-type=e2-medium --zone us-east4-b --subnet vpc-demo-subnet2

      Task 2. Set up a simulated on-premises environment
        //create a VPC network called on-prem:
          gcloud compute networks create on-prem --subnet-mode custom

        //Create a subnet called on-prem-subnet1:
          gcloud compute networks subnets create on-prem-subnet1 \
--network on-prem --range 192.168.1.0/24 --region us-central1
    
        //Create a firewall rule to allow all custom traffic within the network:
          gcloud compute firewall-rules create on-prem-allow-custom \
  --network on-prem \
  --allow tcp:0-65535,udp:0-65535,icmp \
  --source-ranges 192.168.0.0/16

        //Create a firewall rule to allow SSH, RDP, HTTP, and ICMP traffic to the instances:
          gcloud compute firewall-rules create on-prem-allow-ssh-icmp \
    --network on-prem \
    --allow tcp:22,icmp

        //Create an instance called on-prem-instance1 in the region us-central1.
          Note: 
            - In the below command replace with a zone in us-central1 but different from the one used to create the vpc-demo-instance1 in the vpc-demo-subnet1 .
          gcloud compute instances create on-prem-instance1 --machine-type=e2-medium --zone us-central1-b --subnet on-prem-subnet1

      Task 3. Set up an HA VPN gateway
        //create an HA VPN in the vpc-demo network:
          gcloud compute vpn-gateways create vpc-demo-vpn-gw1 --network vpc-demo --region us-central1

        //Create an HA VPN in the on-prem network:
          gcloud compute vpn-gateways create on-prem-vpn-gw1 --network on-prem --region us-central1

        //View details of the vpc-demo-vpn-gw1 gateway to verify its settings:
          gcloud compute vpn-gateways describe vpc-demo-vpn-gw1 --region us-central1

        //View details of the on-prem-vpn-gw1 vpn-gateway to verify its settings:
          gcloud compute vpn-gateways describe on-prem-vpn-gw1 --region us-central1

      Create cloud routers
        //Create a cloud router in the vpc-demo network:
          gcloud compute routers create vpc-demo-router1 \
    --region us-central1 \
    --network vpc-demo \
    --asn 65001

        //Create a cloud router in the on-prem network:
          gcloud compute routers create on-prem-router1 \
    --region us-central1 \
    --network on-prem \
    --asn 65002

    Task 4. Create two VPN tunnels
      Two on-premises VPN gateway devices
        - Each of the tunnels from each interface on the Cloud VPN gateway must be connected to its own peer gateway.
      A single on-premises VPN gateway device with two interfaces
        - Each of the tunnels from each interface on the Cloud VPN gateway must be connected to its own interface on the peer gateway.
      A single on-premises VPN gateway device with a single interface
        - Both of the tunnels from each interface on the Cloud VPN gateway must be connected to the same interface on the peer gateway.

      //Create the first VPN tunnel in the vpc-demo network:
        gcloud compute vpn-tunnels create vpc-demo-tunnel0 \
    --peer-gcp-gateway on-prem-vpn-gw1 \
    --region us-central1 \
    --ike-version 2 \
    --shared-secret [SHARED_SECRET] \
    --router vpc-demo-router1 \
    --vpn-gateway vpc-demo-vpn-gw1 \
    --interface 0

      //Create the second VPN tunnel in the vpc-demo network:
        gcloud compute vpn-tunnels create vpc-demo-tunnel1 \
    --peer-gcp-gateway on-prem-vpn-gw1 \
    --region us-central1 \
    --ike-version 2 \
    --shared-secret [SHARED_SECRET] \
    --router vpc-demo-router1 \
    --vpn-gateway vpc-demo-vpn-gw1 \
    --interface 1

      //Create the first VPN tunnel in the on-prem network:
        gcloud compute vpn-tunnels create on-prem-tunnel0 \
    --peer-gcp-gateway vpc-demo-vpn-gw1 \
    --region us-central1 \
    --ike-version 2 \
    --shared-secret [SHARED_SECRET] \
    --router on-prem-router1 \
    --vpn-gateway on-prem-vpn-gw1 \
    --interface 0

      //Create the second VPN tunnel in the on-prem network:
        gcloud compute vpn-tunnels create on-prem-tunnel1 \
    --peer-gcp-gateway vpc-demo-vpn-gw1 \
    --region us-central1 \
    --ike-version 2 \
    --shared-secret [SHARED_SECRET] \
    --router on-prem-router1 \
    --vpn-gateway on-prem-vpn-gw1 \
    --interface 1

    Task 5. Create Border Gateway Protocol (BGP) peering for each tunnel
      - In this task you configure BGP peering for each VPN tunnel between vpc-demo and VPC on-prem. HA VPN requires dynamic routing to enable 99.99% availability.
      //Create the router interface for tunnel0 in network vpc-demo:
        gcloud compute routers add-interface vpc-demo-router1 \
    --interface-name if-tunnel0-to-on-prem \
    --ip-address 169.254.0.1 \
    --mask-length 30 \
    --vpn-tunnel vpc-demo-tunnel0 \
    --region us-central1
      
      //Create the BGP peer for tunnel0 in network vpc-demo:
        gcloud compute routers add-bgp-peer vpc-demo-router1 \
    --peer-name bgp-on-prem-tunnel0 \
    --interface if-tunnel0-to-on-prem \
    --peer-ip-address 169.254.0.2 \
    --peer-asn 65002 \
    --region us-central1

      //Create a router interface for tunnel1 in network vpc-demo:
        gcloud compute routers add-interface vpc-demo-router1 \
    --interface-name if-tunnel1-to-on-prem \
    --ip-address 169.254.1.1 \
    --mask-length 30 \
    --vpn-tunnel vpc-demo-tunnel1 \
    --region us-central1

      //Create the BGP peer for tunnel1 in network vpc-demo:
        gcloud compute routers add-bgp-peer vpc-demo-router1 \
    --peer-name bgp-on-prem-tunnel1 \
    --interface if-tunnel1-to-on-prem \
    --peer-ip-address 169.254.1.2 \
    --peer-asn 65002 \
    --region us-central1

      //Create a router interface for tunnel0 in network on-prem:
        gcloud compute routers add-interface on-prem-router1 \
    --interface-name if-tunnel0-to-vpc-demo \
    --ip-address 169.254.0.2 \
    --mask-length 30 \
    --vpn-tunnel on-prem-tunnel0 \
    --region us-central1

      //Create the BGP peer for tunnel0 in network on-prem:
        gcloud compute routers add-bgp-peer on-prem-router1 \
    --peer-name bgp-vpc-demo-tunnel0 \
    --interface if-tunnel0-to-vpc-demo \
    --peer-ip-address 169.254.0.1 \
    --peer-asn 65001 \
    --region us-central1

      //Create a router interface for tunnel1 in network on-prem:
        gcloud compute routers add-interface  on-prem-router1 \
    --interface-name if-tunnel1-to-vpc-demo \
    --ip-address 169.254.1.2 \
    --mask-length 30 \
    --vpn-tunnel on-prem-tunnel1 \
    --region us-central1

      //Create the BGP peer for tunnel1 in network on-prem:
        gcloud compute routers add-bgp-peer  on-prem-router1 \
    --peer-name bgp-vpc-demo-tunnel1 \
    --interface if-tunnel1-to-vpc-demo \
    --peer-ip-address 169.254.1.1 \
    --peer-asn 65001 \
    --region us-central1

    Task 6. Verify router configurations
      - In this task you verify the router configurations in both VPCs. You configure firewall rules to allow traffic between each VPC and verify the status of the tunnels. You also verify private connectivity over VPN between each VPC and enable global routing mode for the VPC.

      //View details of Cloud Router vpc-demo-router1 to verify its settings:
        gcloud compute routers describe vpc-demo-router1 \
    --region us-central1

      //View details of Cloud Router on-prem-router1 to verify its settings:
        gcloud compute routers describe on-prem-router1 \
    --region us-central1

    Configure firewall rules to allow traffic from the remote VPC
      - Configure firewall rules to allow traffic from the private IP ranges of peer VPN.
    
      //Allow traffic from network VPC on-prem to vpc-demo:
        gcloud compute firewall-rules create vpc-demo-allow-subnets-from-on-prem \
    --network vpc-demo \
    --allow tcp,udp,icmp \
    --source-ranges 192.168.1.0/24

      //Allow traffic from vpc-demo to network VPC on-prem:
        gcloud compute firewall-rules create on-prem-allow-subnets-from-vpc-demo \
    --network on-prem \
    --allow tcp,udp,icmp \
    --source-ranges 10.1.1.0/24,10.2.1.0/24

    Verify the status of the tunnels
      //List the VPN tunnels you just created:
        gcloud compute vpn-tunnels list

      //Verify that vpc-demo-tunnel0 tunnel is up:
        gcloud compute vpn-tunnels describe vpc-demo-tunnel0 \
      --region us-central1

      //Verify that vpc-demo-tunnel1 tunnel is up:
        gcloud compute vpn-tunnels describe vpc-demo-tunnel1 \
      --region us-central1

      //Verify that on-prem-tunnel0 tunnel is up:
        gcloud compute vpn-tunnels describe on-prem-tunnel0 \
      --region us-central1

      //Verify that on-prem-tunnel1 tunnel is up:
        gcloud compute vpn-tunnels describe on-prem-tunnel1 \
      --region us-central1

    Verify private connectivity over VPN
      //Open a new Cloud Shell tab and type the following to connect via SSH to the instance on-prem-instance1: Replace <zone_name> with the zone in which the on-prem-instance1 was created.
        gcloud compute ssh on-prem-instance1 --zone zone_name

      //From the instance on-prem-instance1 in network on-prem, to reach instances in network vpc-demo, ping 10.1.1.2:
        ping -c 4 10.1.1.2

    Global routing with VPN
      //Open a new Cloud Shell tab and update the bgp-routing mode from vpc-demo to GLOBAL:
        gcloud compute networks update vpc-demo --bgp-routing-mode GLOBAL

      //Verify the change:
        gcloud compute networks describe vpc-demo

      //From the Cloud Shell tab that is currently connected to the instance in network on-prem via ssh, ping the instance vpc-demo-instance2 in region us-east4:
        ping -c 2 10.2.1.2

    Task 7. Verify and test the configuration of HA VPN tunnels
      //Bring tunnel0 in network vpc-demo down:
        gcloud compute vpn-tunnels delete vpc-demo-tunnel0  --region us-central1

      //Verify that the tunnel is down:
        gcloud compute vpn-tunnels describe on-prem-tunnel0  --region us-central1

      //Switch to the previous Cloud Shell tab that has the open ssh session running, and verify the pings between the instances in network vpc-demo and network on-prem:
        ping -c 3 10.1.1.2

    Task 8. (Optional) Clean up lab environment
      //Delete VPN tunnels
        gcloud compute vpn-tunnels delete on-prem-tunnel0  --region us-central1

        gcloud compute vpn-tunnels delete vpc-demo-tunnel1  --region us-central1

        gcloud compute vpn-tunnels delete on-prem-tunnel1  --region us-central1

      //Remove BGP peering
        gcloud compute routers remove-bgp-peer vpc-demo-router1 --peer-name bgp-on-prem-tunnel0 --region us-central1

        gcloud compute routers remove-bgp-peer vpc-demo-router1 --peer-name bgp-on-prem-tunnel1 --region us-central1

        gcloud compute routers remove-bgp-peer on-prem-router1 --peer-name bgp-vpc-demo-tunnel0 --region us-central1

        gcloud compute routers remove-bgp-peer on-prem-router1 --peer-name bgp-vpc-demo-tunnel1 --region us-central1

      //Delete cloud routers
        gcloud compute  routers delete on-prem-router1 --region us-central1

        gcloud compute  routers delete vpc-demo-router1 --region us-central1

      //Delete VPN gateways
        gcloud compute vpn-gateways delete vpc-demo-vpn-gw1 --region "REGION"

        gcloud compute vpn-gateways delete on-prem-vpn-gw1 --region "REGION"

      //Delete instances
        gcloud compute instances delete vpc-demo-instance1 --zone "ZONE"

        gcloud compute instances delete vpc-demo-instance2 --zone "ZONE"

        gcloud compute instances delete on-prem-instance1 --zone zone_name

      //Delete firewall rules
        gcloud compute firewall-rules delete vpc-demo-allow-custom

        gcloud compute firewall-rules delete on-prem-allow-subnets-from-vpc-demo

        gcloud compute firewall-rules delete on-prem-allow-ssh-icmp

        gcloud compute firewall-rules delete on-prem-allow-custom

        gcloud compute firewall-rules delete vpc-demo-allow-subnets-from-on-prem

        gcloud compute firewall-rules delete vpc-demo-allow-ssh-icmp

      //Delete subnets
        gcloud compute networks subnets delete vpc-demo-subnet1 --region "REGION"
        gcloud compute networks subnets delete vpc-demo-subnet2 --region REGION 2
        gcloud compute networks subnets delete on-prem-subnet1 --region "REGION"

      //Delete VPC
        gcloud compute networks delete vpc-demo
        gcloud compute networks delete on-prem
  
  Cloud Interconnect and Peering
    Dedicated 
      - direct connection to google's netwprl
    Shared
      - connection to google's network through a Partner

    Layer 3
      Network layer(Packets)
        - provides access to google to google workspace services/youtube/google api 
          - using public ip addresses
          - using CLoud VPN(Virutal Private Network) service 
            - uses public ip add but traffic is encrypted that provide access to internal IP addr
        - IP
        - router
        - control routing
    Layer 2
      Data link layer(Frames)
        - uses VLAN that pipes directly into your GCP environment 
          - provides connectivity to internal IP add
        - ethernet/mac/llc
        - NIC/Switches
        - error free data transfer
    Types
      Dedicated layer 3
        Direct Peering

      Dedicated layer 2
        Dedicated Interconnect

      Shared layer 3
        Carrier Peering

      Shared layer 3 and 2
        Partner Interconnect

    Cloud Interconnect
      - direct access to RFC1918/ private IP addr/internal IP addr in your VPC with SLA
      VPN tunnel
        - provides encrypted tunnel to VPC networks through the public interconnect
          - IPsecVPN tunnel
          - remote VPN gateway
          - cloud routers
          - BGP peering

        - capacity
          - 1.5 Gbps(public internet) to 3 Gbps(direct peering link) per tunnel

      Dedicated Interconnect
        - provides dedicated, direct connection to VPC networks through google supported olocation facility
        - provides direct physical connections between on-premises and google's network
          - allowing transfer of large amts of data compared for additional BW over public interconnect
        - 99.9% or 99.99% uptime SLA
        - capacity
          - upto 8 links of 10 Gbps
            - min of 10 Gbps
          - upto 10 links 100 Gbps

        how?
          google cloud  share colocation facility with on premise network through BGP
            cloud router to google peering edge between on premise router

        
      Partner Interconnect
        - provides dedicated BW, connection to VPC network through a ISP
        - used when no supported colocation facility near you
        - 99.9% or 99.99% uptime SLA
        - capacity
          - 50 Mbps-50 Gbps per connection

        how?
          google cloud  share colocation facility with supported service provider
            cloud router to google peering edge between on service provider peering edge to on pre,ise router

        Process
          - Identify supported locations
          - choose either primary/redundant cross-cloud interconnect ports from google and service provider
       
      Cross-cloud interconnect
        - provides dedicated physical connection between VPC network and network hosted by cloud service provider
        - estabilish high-BW dedicated connectivity bet googl cloud and another cloud service provider
        - supports the adoption of an integrated multi cloud strategy
        - 2 connection sizes:
          - 10 Gbps
          - 100 Gbps
        - google provision a dedicated physical connection 

      Note:
        - Choose Cloud VPN
          - lower cost solution
          - lower BW needs
          - experimenting w/ migrating workloads to Google CLoud
        - Choose Dedicated or Parner Interconnect
          - enterprise grade connection that has higher throughput
        - Choose Cross-Cloud Interconnect
          - connect to another cloud service provider
    
    Peering
      - useful when access to google cloud propertions
      - access  to google public IPs only w/o SLA
      Types
        1. Direct peering
          - allows connection between your business network and google's using broad-reaching edge network locations
            - exchange BGP routes bet google and peering entity   
          - No SLA
          - Peering requirements
            - https://peering.google.com/#/options/peering
            - GCP edge Points of Presence( PoPs )
              - where google's network connects to the rest of the internet via peering
              - https://www.peeringdb.com/asn/15169
        
        2. Carrier peering
          - used when require access to google poublic infra and cannot satisfy google's peering requirements
          - works/ direction with carrier peering Partner
          - no SLA
          - support Internet service provider and requirments
            - https://cloud.google.com/network-connectivity/docs/carrier-peering#service_providers
    
    Choosing a connection
      If connect on-premises to Google Workspace/Google APIs
        - choose Direct peering'
          - meet google peering requirments
        - chose carrier peering
          - didn't meet peering requirments

      If connect google cloud wuth another cloud service
        - Choose Cloud VPN
          - connect google cloud VPC networks w/ other cloud
          - don't need dedicated high BW
          - encryption managed by google
        - Choose Self-managed encryption tunnels
          - connect google cloud VPC networks w/ other cloud
          - don't need dedicated high BW
          - not encryption managed by google
        - Choose Cross-Cloud Interconnect
          - connect google cloud VPC networks w/ other cloud
          - need dedicated high BW
          - Google managed routing solution
        - Choose Self-managed connectivity
          - connect google cloud VPC networks w/ other cloud
          - need dedicated high BW
          - not google managed routing solution

      If connect on-premises w/ google cloud
        Choose Dedicated Interconnect
          - need dedicated and high BW
          - min dedicated BW of 10 Gbps
        Choose Parner Interconnect
          - need dedicated and high BW
          - not minimum dedicated BW of 10 Gbp
            Choose L2 Partner Interconnect
              - need BGP peering
            Choose L3 Partner Interconnect
              - no BGP peering
        Choose Cloud VPN
          - does not need dedicated and high BW
          - encryption managed by google
        Choose Self-managed encryption tunnels
          - don't need dedicated high BW
          - not encryption managed by google
        Choose Cloud VPN over Cloud Interconnect 
          - with BW of more 10 Gbps
          - need dedicated and high network BW
          - encryption managed by google
  
  Sharing VPC Networks
    Shared VPC and VPC Peering
      Shared VPC 
        - allows share a network across several projects in same GCP org using Internal IP
      
      Shared VPC Peering/VPC network peering
        - allow to private IP/internal IP connectivity across two VPC networks across diff projects/org
        - a decentralized/distributed approach to multi-project networking
        - doesn' incur:
          - network latency
          - security
          - cost drawbacks
    
    Note:
      Host/service project  
        - project that participates in shared VPC

      Standalone project
        - project does not participate in shared VPC

      Standalone VPC network
        - unshared VPC network that exists in eitheri standalone project or a servic prokect
  Quiz
    1. What is the purpose of Virtual Private Networking (VPN)?
      - To enable a secure communication method (a tunnel) to connect two trusted environments through an untrusted environment, such as the Internet.
        - VPN use IPsec tunnels to provide encapsulated and encrypted path through a hostile or untrusted environment
    2. Which Google Cloud Interconnect service requires a connection in a Google Cloud colocation facility and provides 10 Gbps per link?
      - Dedicated Interconnect
    3. If you cannot meet Googleâ€™s peering requirements, which network connection service should you choose to connect to Google Workspace and YouTube?
      - Carrier Peering
    4. Which of the following approaches to multi-project networking, uses a centralized network administration model?
      - Shared VPC
        - centralized approach to multi-project networking because securiy and networking policy occurs in single designated VPC networkx

Module 2 - Load Balancing and Autoscaling
  - Configure load balancers and autoscaling for VM instances

  Learning Objectives
    1. Recall the various load balancing services
    2. Determine which Google Cloud load balancer to use in specific circumstances
    3. Describe autoscaling behavior, policies, configuration, and metrics
    4. Configure load balancers and autoscaling
    
  Managed Instance Groups
    - a collection of identical VM instances that you control as a single entity using an instance template
    - deploy identical instances based on instance template
    - instance group can be resized
    - manager ensures all instances are running
    - typically used w/ autoscaler
    - can be single zone or regional

    Autoscaling and health checks
      - allows you to auto add/remove instances from a MIG based on load

      Autoscaling policies supports
        - CPU utilization
        - load balancing capacity
        - monitoring metrics
        - queue-based workload like Pub/Sub 
        - schedule such as start-time, duration and recurrence

      Health checks
        - similar to uptime check in stackdriver

        Setup?
          - Protocol
          - ports
          - health criteria:
              1. timeout_sec
                - How long to wait (in seconds) before a request is considered a failure
              2. check_interval_sec
                - how often in seconds to check whether an instance is healthy
              3. healthy_threshold
                - How many consecutive successes must occur to mark a VM instance healthy.
              4 .unhealthy_threshold
                - How many consecutive failures must occur to mark a VM instance unhealthy

  HTTP(S) Load Balancing
    - acts at layer7/application layer of OSI model
      - deals w/ actual content of each message and allows routing decision based on unhealthy_threshold
    - provides global load balancing
    - anyacast IP addr
    - HTTP on port 80/8080
    - HTTPs on port 443
    - supports ipv4 and ipv6 clients
    - autoscaling
      - no pre-warming and enables content-based and cross regiona load balamncing
    - url maps
      - route some URLs to one set of instance like distributing them
      - req are generally routed to instance group near to the user 
    
    Architecture of an HTTP load balancer
      Internet -> Global Forwarding rule -> Target Proxy -> URL map
      -> Backend Service(Constantly checked by Health Check) -> Instance Group -> 
    
      Global Forwarding rule
        - directs incoming requests from the internet to a target HTTP Proxy.
      
      Target HTTP Proxy
        - checks each request against a URL map to determine appropriate backend service

      Backend Service
        - directs each request to an appropriate instance  based on serving capacity, zone/location and instance health
        - contains:
          - health check
            - only instances that pass health check are allowed to receive new requests
          - sesson affinity
            - attempts to send all req from the same client to same virtual machine instance
          - timeout settings(30 sec by default)
            - amt of time backend service will wait on backend before considering req is a failure
          - one or more backends
            - an instance group(managed or unmanaged)
            - a balancing mode(CPU utilization or RPS)
              - tells load balancing sys how to det when backend is at full usage
            - A capacity scaler(ceiling percentage of CPU/Rate targets)
              - additional control w/ balancing mode settings
                - 80% CPU utilization and 100% capacity
                - 80% CPU utilization and 50% capacity

    Distributing Algorithm used are
      1. Round robin
        - use by default to distribute requests among available instances.

    Cross-Region  LB
      - backend service in 2 or more region

    Content-Based LB
      - url map split traffic based on content(video/web traffic)

    HTTPS LB
      - same structure of HTTP LB but uses target HTTPS proxy instead of HTTP proxy.
      - reuires at least one signed SSL cert installed
        - upto 15 SSL cert per target proxy
      - client SSL session terminates at the LB
      - support the QUIC transport layer Protocol
        QUIC transport layer Protocol
          - quick UPD internet connections allows faster client connections
          - lower latncy because eliminates head of line blocking multiplexed stream
          - supports connection migration when client's IP addr changes

      Network endpoint groups(NEG)
        - a config object that specifies a group of backend endpoints/services
          - cloud run/app engine/function
            - serverless neg means dno endpoints
          Internet NEG
            - contains single endpoint  specified by IP addr hosted outside google cloud

    Qwiklab: Configuring an HTTP load balancer w/ autoscaling
      https://googlecoursera.qwiklabs.com/focuses/34883910?parent=lti_session
      - Google Cloud HTTP(S) load balancing is implemented at the edge of Google's network in Google's points of presence (POP) around the world. 
        User traffic directed to an HTTP(S) load balancer enters the POP closest to the user and is then load-balanced over Google's global network to the closest backend that has sufficient available capacity.

      - In this lab, you configure an HTTP load balancer as shown in the diagram below. 
        Then, you stress test the load balancer to demonstrate global load balancing and autoscaling.

      Objectives
      In this lab, you learn how to perform the following tasks:

        1. Create a health check firewall rule
        2. Create a NAT configuration using Cloud Router
        3. Create a custom image for a web server
        4. Create an instance template based on the custom image
        5. Create two managed instance groups
        6. Configure an HTTP load balancer with IPv4 and IPv6
        7. Stress test an HTTP load balancer

      The HTTP load balancer should forward traffic to the region that is closest to you.
        - yes

      //To place a load on the load balancer, run the following command:
      ab -n 500000 -c 1000 http://$LB_IP/
    
  Cloud CDN(Content Delivery Network)
    - cached content at CDN nodes/Edge PoPs near to your users for faster response

    Cloud CDN cache modes
      - control the factors that determine whether or not Cloud CDN caches your content

      Three chache modes:
        1. USE_ORIGIN_HEADERS
          - requires origin responses to set valid cache directives and valid caching headers
        
        2. CACHE_ALL_STATIC
          - auto caches static content that doesn't have no-store,private or no cache directive
        
        3. FORCE_CACHE_ALL
          - unconditionally caches responses, overriding any cache directives set by the origin

      Note:
        - not to cache:
          - private data
          - per user content such as dynamic HTML or API responses

  SSL Proxy/TCP Proxy Load Balancing
    SSL Proxy Load Balancing
      - global load balancing for encrypted non HTTP traffic
      - terminates ssl session at load balancing layer
      - multi region and auto directs traffic to closest region that has capacity
      - supports ipv4 and ipv6 addr for client traffic
      
      Benefits:
        - Intelligent routing
          - route req to backend loc where there is capacity
        - Certificate mngmt
          - you only need to update customer facing cert in one place when you need to switch those certificates
          - can reduce mngmt overhead by using self-signed cert on your instances
        - security patching
          - auto apply patches with SSL/TCP stack is vulnerable
        - SSL policies
          - https://cloud.google.com/load-balancing/docs/tcp#overview

    TCP Proxy Load Balancing
      - global load balancing for encrypted non HTTP traffic
      - terminates ssl session at load balancing layer
      - supports ipv4/ipv6 clients

      Benefits:
        - Intelligent routing
          - route req to backend loc where there is capacity
        - security patching
          - auto apply patches with SSL/TCP stack is vulnerable

  Network Load Balancing
    - a regional non-proxied LB.
      - traffic can only be balanced bet VM instances in same region
    - only supports ipv4 clients
    - Forwarding rules(IP protocol data)
      - balance load of your sys based on the incoming IP protocol data
        - addr
        - port
        - protocol type
    - Traffic:
      - UDP
      - TCP/SSL ports
    - Architecture:
      - Backend service-based
        - regional backend service
        - defines the behavior of the load balancer and how it distributes traffic to its backend instance groups
        - enables new features not supported w/ legacy target pools
          - non-legacy health checks
            - TCP
            - SSL
            - HTTP
            - HTTPS
            - HTTP/2
          - auto-scaling w/ managed instance groups
          - connection draining
          - configurable failover policy
      - Target pool-based
        - defines a group of instances that receive incoming traffic from forwarding rules
          - TCP and UDP traffic only
        - balance traffic based on a hash of the source IP and port and the dest IP and port
        - up to 50 target pools per project in the same region
        - one health check

  Internal Load Balancing
    Internal TCP/UDP  LB
      - regional private LB
        - VM instance in same region
        - private IP add/internal IP addr
      - TCP/UDP traffic
      - reduced latency, simpler configuration
      - software-defined fully distributed LB
        - directly delivers the traffic from client isntance to a backend instance using oogle network virtualizaiton  stack Andromeda

    Internal HTTP(S) LB
      - proxy based regional layer 7 private LB
        - VM instances in same region
        - private IP addr
      - Backend services supports HTTP/HTTPS/HTTP/2 products
      - Based on open source envoy proxy
        - manage and balance incoming network traffic

    
    Qwilkab: Configuring and Internal Load balancer
      - https://googlecoursera.qwiklabs.com/focuses/34907383?parent=lti_session
      - Google Cloud offers Internal Load Balancing for your TCP/UDP-based traffic. 
        Internal Load Balancing enables you to run and scale your services behind a private load balancing IP address that is accessible only to your internal virtual machine instances.
      
      Objectives
      In this lab, you will learn how to perform the following tasks:

        1. Create internal traffic and health check firewall rules.
        2. Create a NAT configuration using Cloud Router.
        3. Configure two instance templates.
        4. Create two managed instance groups.
        5. Configure and test an internal load balancer

      Which of these fields identify the location of the backend?
        -Server Hostname
        -Server Location


  Choosing a load balancer
    1. Application LB
      - use for applications w/ HTTP(S) traffic.

      a. external
      b. Internal
    2. Network LB
      - - use for applications w/ TCP/UDP/SSL/TLS/Layer 4 protocols
      a. Proxy network LB
        - implement SSL/TLS offload, TC/UDP proxy or support external LB to backends in multiple regions
      b. Passthrough network LB
        - preserve clietn source IP addr, avoid overhead of proxies and supoort UDP/ESP/ICMP.
          - use UDP if you need to expose client IP addr to your applications
  
  Quiz
  1. Which of the following is not a Google Cloud load balancing service?
    - hardware-defined LB
      - cloud LB is managed service for all traffic and not device based sol'm
  2. Which three Google Cloud load balancing services support IPv6 clients?
    - HTTP(S) LB
    - SSL/TCP proxy LB
    - 
  3. Which of the following are applicable autoscaling policies for managed instance groups?
    - Load balancing capacity
    - Queue-based workload
    - Monitoring metrics
    - CPU utilization

  Review
    - combine external and internal LV to support 3 tier web services

Module 3 - Infrastructure Automation
  - Automate the deployment of Google Cloud infrastruc

  Learning Objectives
    1. Automate the deployment of Google Cloud services using Terraform
    2. Outline the Google Cloud Marketplace

  Terraform
    - allows quick provisioning and removing infra safely
    - uses a sys of highly structured templates and configuration files to document the infra in an easily readabale and understandable format
    - conceals the actual cloud API calls
      - don't need to write api calls only infra code
    - enables you to safely and predicatably create, change and improve infra
    - an open-source tool that codifies APIs into declarative configuration files that can be shared among team members, treated as code, edited, reviewed, and versioned
    - management tool for serveral resources that needs repeatable way to deploy, update and destroy uniformly.
      - build infra when needed/repeatable deployment Process
      - declarive language
      - parallel deployment
      - template driven
      - destroy infra when not in use
      - create identical ifnra for dev/staging/pro
      - can be part of CI/CD pipeline
    - templates are building blocks for DR procedures
    -  fully managed resource dependencies and complexity
    
    Terraform language
      - hashicorp Configuration language(HCL)
      - interface to declare resources

      Components
        Block type
        Block Label resource name
          - defines resource/services name
        Block Label name
          - defines custom pointer name of created resources
        Block Body
          - contains variable and argument
            - identifier name
            - expression

    Qwiklab: Automating the infrastructure of networks using Terraform
      https://googlecoursera.qwiklabs.com/focuses/34926833?parent=lti_session

      Objectives
      In this lab, you learn how to perform the following tasks:

        1. Create a configuration for an auto mode network
        2. Create a configuration for a firewall rule
        3. Create a module for VM instances
        4. Create and deploy a configuration
        5. Verify the deployment of a configuration
  
  Google Cloud Marketplace
    - quicly deploy functional software packages that run on Google Cloud
    - production-grade solutions from third-party vendors who have already created their own deployment configurations based on Terraform
    - single bill for google cloud and third party services
    - notif when a security update is available
    - direct access to partner support

  Quiz
    1. Whatâ€™s the benefit of writing templates for your Terraform configuration?
      - Allows you to abstract part of your configuration into individual building blocks that you can reuse
      - reusable code across deployments
    2. What does Google Cloud Marketplace offer?
      - Production-grade solutions from third-party vendors who have already created their own deployment configurations based on Terraform

Module 4 - Managed Services
  - Leverage managed services in Google Cloud
  - exist between PaaS and SaaS

  Learning Objectives
    1. Describe the managed services for data processing in Google Cloud

  BigQuery/AWS Redshift
    - serverless highly scalable cost-effective cloud entreprise data warehouse
      - fully managed by google
        - you can focus analyzing your data
      - up to petabyte scale
      - SQL interface
      - very fast queries
    - designed to help you ingest, store, analyze and visualize data w/ ease
    
    Features
      - Ingest Data
        - upload in batch
        - streaming direclty as in real time insights
      
      - built-in AI/Vertex AI/Generative AI
        - turn complex data into compelling understandable stories

    how to use
      - Cloud console
      - CLI
      - REST API
        - Java
        - nodejs
        - python

    Pricing
      - pay for data storage, streaming inserts and querying data
        Storage data
          - amt of storage
        Querying data
          - flat rate for dedicated resources
          - pay per query
      Free of charge:
        - loading and exporting data

    How to find out more?
      https://cloud.google.com/bigquery?hl=en
  
  Dataflow/AWS cloud dataflow
    - a serverless, fast and cost effective data-processing service for stream and batch data.
      - removes operational overhead by automating infra provisioning and auto scaling as your data grows.
    - a fully managed streaming analytics service that minimizes latency, processing time and cost rhough autoscaling and batch processing.
    - process, enrich,  stream and batch data 
    - open source programming using Beam
    - intelligently scale to millions of QPS

    Integrates with:
      - stackdriver

    Process:
      - Read Data Source as input -> Transform -> write it back into a Data Sink
      - ingest -> process -> analyze
      - extract -> trasnform -> load
  
    Usecase
      - real-time AI
      - data warehousing
      - stream analysis

  Dataprep by Trifecta
    - transforms raw data into ready-to-use data pipelines for your analytics or machine learning.
    - can discover data structure and patterns surfaces data quality issues and provides guidance to resolve it
    - serverless intelligent data service for visually exploring, cleaning and preparing structured/unstructured data for analysis reporting and machine learning
      - auto scaling
      - suggests ideal data transformation
      - focus on data analysis
      - integrated partner service oeprated by trifecta

    More info
      https://cloud.google.com/dataprep

    Dataprep Architecture
      Ingestion -> preparation & storage -> analysis & ML
  
  Dataproc
    - a service for running apache spack and apache hadoop clusters
      - low cost(per-second, preemptible)
      - super fast to start, scale and shut down
        - 90 seconds each operations
      - integrated w/ google cloud
        - bigquery
        - cloud storage
        - cloud bigtable
        - stackdriver logging and monitoring
    - managed data processing service for any open source data tools like:
      - apache spark
      - apache hadoop clusters
      - flink
      - presto
      - pig
      - hive
    
    Usecase
      - batch processing
      - SQL
      - streaming
      - Machine learning
      - move on premise OSS clusters to the cloud
      - to maximize efficiency and enable scale
      - can use w/ cloud ai notebook/bigquery to build data science environment
      - spin up an IT governed, auto scaling cluster in 90 seconds

    Pricing
      - pay what you use

    Dataflow vs Dataproc
      Choose Dataflow
        - not dependent on apache hadoop/spark
        - auto provisioning of clusters
        - serverless approach
      Choose Dataproc
        - manual provisioning clusters
        - devops approach
        - either dependent not dependent on apache hadoop/spark

    Quiz
    1. How are Managed Services useful?
      - Managed Services may be an alternative to creating and managing infrastructure solutions.
        - possible alternative to building your own infrastructure data processing solution.

    2. Which of the following is a feature of Dataproc?
      - It typically takes less than 90 seconds to start a cluster.

